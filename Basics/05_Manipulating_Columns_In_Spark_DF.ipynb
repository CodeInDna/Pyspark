{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb06aa54",
   "metadata": {},
   "source": [
    "# Processing Column Data\n",
    "\n",
    "We will explore the functions available under pyspark.sql.functions to derive new values from existing column values within a dataframe.\n",
    "* [Pre-defined Functions](#first)\n",
    "* [Create dummy dataframe](#sec)\n",
    "* [Categories of functions](#third)\n",
    "* [Special fucntions - col and lit](#fourth)\n",
    "* [String manipulation functions - 1](#fifth)\n",
    "* [String manipulation functions - 2](#sixth)\n",
    "* [Date and Time overview](#seventh)\n",
    "* [Date and Time arithmetic](#eigth)\n",
    "* [Date and Time - trunc and date_trunc](#ninth)\n",
    "* [Date and Time - extracting information](#tenth)\n",
    "* [Dealing with UNIX timestamp](#eleventh)\n",
    "* [Dealing with Nulls](#twelth)\n",
    "* [Conclusion](#thirteen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc39dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78998558",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkFunctions').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ebf74",
   "metadata": {},
   "source": [
    "### Pre-defined Functions<a id=\"first\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace95291",
   "metadata": {},
   "source": [
    "We typically process data in the columns using functions in `pyspark.sl.functions`. Let us understand details about these functions in detail as part of this module.\n",
    "\n",
    "* Let us recap about functions or API to proces DataFrames.\n",
    "    * Projection - `select` or `withColumn` or `drop` or `selectExpr`\n",
    "    * Filtering - `filter` or `where`\n",
    "    * Grouping data by key and perform aggregations - `groupBy`\n",
    "    * Sorting data - `sort` or `orderBy`\n",
    "* We can pass column names or literals or expressions to all the dataframe APIs.\n",
    "* Expressions include arithmetic operations, transformations using functions from `pyspark.sql.functions`.\n",
    "* There are approximately 300 functions under `pyspark.sql.functions`.\n",
    "* There are some important functions related to String Manipulation, Date Manipulation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54af4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('../data/orders.csv', \n",
    "        schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76f2c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59f7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eeb17",
   "metadata": {},
   "source": [
    "#### date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce64bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "        specialized implementation.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7a412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dateformat with select and alias\n",
    "orders.select('*',\n",
    "             date_format('order_date', 'yyyyMM').alias('order_month')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60ce72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dateformat with withColumn\n",
    "orders.withColumn('order_month', date_format('order_date', 'yyyyMM')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd6bf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ce386b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6e40a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------+\n",
      "|      date|                time|date_yd|time_yd|\n",
      "+----------+--------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:...|2014059|2014059|\n",
      "|2016-02-29|2016-02-29 08:08:...|2016060|2016060|\n",
      "|2017-10-31|2017-12-31 11:59:...|2017304|2017365|\n",
      "|2019-11-30|2019-08-31 00:00:...|2019334|2019243|\n",
      "+----------+--------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "withColumn(\"date_yd\", date_format(\"date\", \"yyyyDDD\").cast(\"int\")). \\\n",
    "withColumn(\"time_yd\", date_format(\"time\", \"yyyyDDD\").cast(\"int\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c4de301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+--------------+--------------+\n",
      "|      date|                time|          date_yd|date_name_abbr|date_name_full|\n",
      "+----------+--------------------+-----------------+--------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|February 28, 2014|           Fri|        Friday|\n",
      "|2016-02-29|2016-02-29 08:08:...|February 29, 2016|           Mon|        Monday|\n",
      "|2017-10-31|2017-12-31 11:59:...| October 31, 2017|           Tue|       Tuesday|\n",
      "|2019-11-30|2019-08-31 00:00:...|November 30, 2019|           Sat|      Saturday|\n",
      "+----------+--------------------+-----------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF. \\\n",
    "withColumn(\"date_yd\", date_format(\"date\", \"MMMM d, yyyy\")). \\\n",
    "withColumn(\"date_name_abbr\", date_format(\"date\", \"EE\")). \\\n",
    "withColumn(\"date_name_full\", date_format(\"date\", \"EEEE\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346c78f",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adcefdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class filter in module builtins:\n",
      "\n",
      "class filter(object)\n",
      " |  filter(function or None, iterable) --> filter object\n",
      " |  \n",
      " |  Return an iterator yielding those items of iterable for which function(item)\n",
      " |  is true. If function is None, return the items that are true.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee5e32",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbded0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupBy in module pyspark.sql.dataframe:\n",
      "\n",
      "groupBy(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Groups the :class:`DataFrame` using the specified columns,\n",
      "    so we can run aggregation on them. See :class:`GroupedData`\n",
      "    for all the available aggregate functions.\n",
      "    \n",
      "    :func:`groupby` is an alias for :func:`groupBy`.\n",
      "    \n",
      "    :param cols: list of columns to group by.\n",
      "        Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "    \n",
      "    >>> df.groupBy().avg().collect()\n",
      "    [Row(avg(age)=3.5)]\n",
      "    >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "    [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orders.groupBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87dbb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "|   25878|2014-01-01 00:00:...|             9084|        PENDING|\n",
      "|   25879|2014-01-01 00:00:...|             5118|        PENDING|\n",
      "|   25880|2014-01-01 00:00:...|            10146|       CANCELED|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function as a part of where or filter\n",
    "orders.filter(date_format('order_date', 'yyyyMM') == 201401).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0401c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "|     201312| 5892|\n",
      "|     201310| 5335|\n",
      "|     201311| 6381|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function as a part of groupBy\n",
    "orders.groupBy(date_format('order_date', 'yyyyMM').alias('order_month')).count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3b917",
   "metadata": {},
   "source": [
    "### Create dummy dataframe<a id=\"sec\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89db5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X', )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112b65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, 'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e2d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "330c2329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3808b3",
   "metadata": {},
   "source": [
    "Once dataframe is created, we can use to understand how to use functions. for examples, to get current date, we can run `df.select(current_date()).show()`\n",
    "\n",
    "It is similar to Oracle Query `SELECT sysdate FROM dual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2939b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2022-01-27|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias('current_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0eeea",
   "metadata": {},
   "source": [
    "Here is the example of creating dataframe using collection of employees. we will be using this dataframe to explore all the important functions to process column data in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d453f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", 'Tiger', 1000.0, \"USA\", \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "            (2, \"Henry\", 'Ford', 750.0, \"UK\", \"+1 123 456 7890\", \"123 45 6789\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "361ff2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF = spark.createDataFrame(employees, schema=\"\"\"id INT, f_name STRING, l_name STRING, salary FLOAT, nationality STRING, ph_no STRING, ssn STRING\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95e4d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- f_name: string (nullable = true)\n",
      " |-- l_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- ph_no: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41143ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|id |f_name|l_name|salary|nationality|ph_no          |ssn        |\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|1  |Scott |Tiger |1000.0|USA        |+1 123 456 7890|123 45 6789|\n",
      "|1  |Henry |Ford  |750.0 |UK         |+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a810d7",
   "metadata": {},
   "source": [
    "### Categories of functions<a id=\"third\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f89f9",
   "metadata": {},
   "source": [
    "There are approx 300 functions under `pyspark.sql.functions`. At a higher level they canbe grouped into a few categories.\n",
    "\n",
    "* String Manipulation\n",
    "    * Case conversion - `lower`, `upper`\n",
    "    * Getting length - `length`\n",
    "    * Extracting substrings - `substring`, `split`\n",
    "    * Trimming - `trim`, `ltrim`, `rtrim`\n",
    "    * Padding - `lpad`, `rpad`\n",
    "    * Concatenating string - `concat`, `concat_ws`\n",
    "* Date Manipulation\n",
    "    * Getting current date and time - `current_date`, `current_timestamp`\n",
    "    * Date arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n",
    "    * Begining and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n",
    "    * Formatting Date - `date_format`\n",
    "    * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n",
    "* Aggregate Functions\n",
    "    * `count`, `countDistinct`\n",
    "    * `sum`, `avg`\n",
    "    * `min`, `max`\n",
    "* Other Functions - We will explore depending on the use cases.\n",
    "    * `CASE` and `WHEN`\n",
    "    * `CAST` for type casting\n",
    "    * Functions to manage special types such as `ARRAY`, `MAP`, `STRUCT` type columns\n",
    "    * Many others "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7774e",
   "metadata": {},
   "source": [
    "### Special functions - col and lit<a id=\"fourth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d584852",
   "metadata": {},
   "source": [
    "* For dataframe APIs such as `select`, `groupBy`, `orderBy` etc we can pass column names as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c236ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|f_name|l_name|\n",
      "+------+------+\n",
      "| Scott| Tiger|\n",
      "| Henry|  Ford|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(\"f_name\", \"l_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48564a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|nationality|count|\n",
      "+-----------+-----+\n",
      "|        USA|    1|\n",
      "|         UK|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af2e1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "| id|f_name|l_name|salary|nationality|          ph_no|        ssn|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|  2| Henry|  Ford| 750.0|         UK|+1 123 456 7890|123 45 6789|\n",
      "|  1| Scott| Tiger|1000.0|        USA|+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f4c98",
   "metadata": {},
   "source": [
    "* If there are no transformations on any column in any function then we should be able to pass all column names as strings.\n",
    "* If not, we need to pass all columns as type column by col function.\n",
    "* If we need to apply any transformation using functions then passing column names as strings to some of the functions will not suffice. We have to pass them as column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b968f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(upper(col(\"f_name\")), upper(col(\"l_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be5c8b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Approach 1\n",
    "employeeDF.select(upper(employeeDF[\"f_name\"]), upper(employeeDF[\"l_name\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7626ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Approach 2\n",
    "employeeDF.select(upper(employeeDF[\"f_name\"]), upper(employeeDF[\"l_name\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200da170",
   "metadata": {},
   "source": [
    "* Also, if we want to use functions such as `alias`, `desc` etc on columns then we have to pass the column names as column type(not as strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88b4bb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9628/2895053244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memployeeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(\"id\".desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "863817cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "| id|f_name|l_name|salary|nationality|          ph_no|        ssn|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|  2| Henry|  Ford| 750.0|         UK|+1 123 456 7890|123 45 6789|\n",
      "|  1| Scott| Tiger|1000.0|        USA|+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(col(\"id\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf4a37",
   "metadata": {},
   "source": [
    "* Sometimes, we want to add a literal to the column values. For e.g., we might want to concatenate first_name and last_name seperated by comma and space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bd21d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [f_name, id, l_name, nationality, ph_no, salary, ssn];;\n'Project [concat(f_name#447, ', , l_name#448) AS concat(f_name, , , l_name)#649]\n+- LogicalRDD [id#446, f_name#447, l_name#448, salary#449, nationality#450, ph_no#451, ssn#452], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9628/2027930771.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memployeeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"l_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [f_name, id, l_name, nationality, ph_no, salary, ssn];;\n'Project [concat(f_name#447, ', , l_name#448) AS concat(f_name, , , l_name)#649]\n+- LogicalRDD [id#446, f_name#447, l_name#448, salary#449, nationality#450, ph_no#451, ssn#452], false\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(concat(col(\"f_name\"), \", \", col(\"l_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1021f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(concat(col(\"f_name\"), lit(\", \"), col(\"l_name\")).alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0fa17",
   "metadata": {},
   "source": [
    "### String manipulation functions - 1<a id=\"fifth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741791df",
   "metadata": {},
   "source": [
    "Let us understand how to concatenate strings using concat function.\n",
    "\n",
    "* Concatenating strings\n",
    "    * We can pass a variable number of strings to `concat` function.\n",
    "    * It will return one string concatenating all the strings.\n",
    "    * If we have to concatenate literal in between then we have to use lit function.\n",
    "\n",
    "* Case Conversion and Length\n",
    "    * Convert all the alphabetic characters in a string to uppercase - `upper`\n",
    "    * Convert all the alphabetic characters in a string to lowercase - `lower`\n",
    "    * Convert first character in a string to uppercase - `initcap`\n",
    "    * Get number of characters in a string - `length`\n",
    "    * All the 4 functions take column type argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7366efc",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "Let us perform few tasks to understand more about concat function.\n",
    "\n",
    "* Letâ€™s create a Data Frame and explore concat function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e337a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d903491",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fffea62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708c378",
   "metadata": {},
   "source": [
    "* Create a new column by name `full_name` concatenating `first_name` and `last_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7691ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b962e",
   "metadata": {},
   "source": [
    "* Improvise by adding a **comma followed by a space** in between **first_name** and **last_name**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6538eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", lit(', '), \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a33ec6",
   "metadata": {},
   "source": [
    "#### Case Conversion and Length\n",
    "\n",
    "* Use employees data and create a Data Frame.\n",
    "* Apply all 4 functions on **nationality** and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d2a5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'nationality') \\\n",
    ".withColumn(\"nationality_upper\", upper(col('nationality'))) \\\n",
    ".withColumn(\"nationality_lower\", lower(col('nationality'))) \\\n",
    ".withColumn(\"nationality_initcap\", initcap(col('nationality'))) \\\n",
    ".withColumn(\"nationality_length\", length(col('nationality'))) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89753c67",
   "metadata": {},
   "source": [
    "####  Substring\n",
    "\n",
    "Let us understand how we can extract substrings using function  `substring`\n",
    "* If we are processing **fixed length columns** then we use `substring` to extract the information.\n",
    "* Here are some of the examples for **fixed length columns** and the use cases for which we typically extract information.\n",
    "* 9 Digit Social Security Number. We typically extract last 4 digits and provide it to the tele verification applications.\n",
    "* 16 Digit Credit Card Number. We typically use first 4 digit number to identify Credit Card Provider and last 4 digits for the purpose of tele verification.\n",
    "* Data coming from MainFrames systems are quite often fixed length. We might have to extract the information and store in multiple columns.\n",
    "* `substring` function takes 3 arguments, **column**, **position**, **length**. We can also provide position from the end by passing negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21779349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str, pos, len)\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. note:: The position is not zero based, but 1 based index.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bf5f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X', )]\n",
    "df = spark.createDataFrame(l, \"dummy STRING\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3141d589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), 7, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46b6a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), -5, 5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301e397",
   "metadata": {},
   "source": [
    "#### Tasks - substring\n",
    "\n",
    "Let us perform few tasks to extract information from fixed length strings.\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "99d20684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'phone_number', 'ssn'). \\\n",
    "            withColumn('phone_last4', substring(col('phone_number'), -4, 4).cast(\"int\")). \\\n",
    "            withColumn('ssn_last4', substring(col('ssn'), -4, 4).cast(\"int\")). \\\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cbdbe2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- phone_last4: integer (nullable = true)\n",
      " |-- ssn_last4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'phone_number', 'ssn'). \\\n",
    "            withColumn('phone_last4', substring(col('phone_number'), -4, 4).cast(\"int\")). \\\n",
    "            withColumn('ssn_last4', substring(col('ssn'), -4, 4).cast(\"int\")). \\\n",
    "            printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "977fd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef2be5",
   "metadata": {},
   "source": [
    "#### Extracting Strings using split\n",
    "Let us understand how we can extract substrings using  `split`.\n",
    "\n",
    "* If we are processing **variable length columns** with **delimiter** then we use `split` to extract the information.\n",
    "* Here are some of the examples for **variable length columns** and the use cases for which we typically extract information.\n",
    "* Address where we store House Number, Street Name, City, State and Zip Code comma separated. We might want to extract City and State for demographics reports.\n",
    "* `split` takes 2 arguments, **column** and **delimiter**.\n",
    "* `split` convert each string into array and we can access the elements using index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e1c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9841463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)|\n",
      "+--------------------------------------+\n",
      "|[Hello, World,, how, are, you]        |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810bd56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)[2]|\n",
      "+-----------------------------------------+\n",
      "|how                                      |\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")[2]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d536b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|word                          |\n",
      "+------------------------------+\n",
      "|[Hello, World,, how, are, you]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \").alias('word')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cd379",
   "metadata": {},
   "source": [
    "* Most of the problems can be solved either by using `substring` or `split`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c17eee",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "Let us perform few tasks to extract information from fixed length strings as well as delimited variable length strings.\n",
    "\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract area code and last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a7490f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111, +44 111 111 1123\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210, +61 987 654 3223\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d60ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_numbers STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a2ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|employee_id|       phone_numbers|\n",
      "+-----------+--------------------+\n",
      "|          1|     +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|\n",
      "|          4|+61 987 654 3210,...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"phone_numbers\") \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8ab30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------------+\n",
      "|employee_id|       phone_numbers|        ssn|    phone_number|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "|          1|     +1 123 456 7890|123 45 6789| +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"phone_numbers\", \"ssn\") \\\n",
    "            .withColumn(\"phone_number\", explode(split(col(\"phone_numbers\"), \", \"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45964d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = employeesDF.select(\"employee_id\", \"phone_numbers\", \"ssn\") \\\n",
    "            .withColumn(\"phone_number\", explode(split(col(\"phone_numbers\"), \", \"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4755b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------------+\n",
      "|employee_id|       phone_numbers|        ssn|    phone_number|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "|          1|     +1 123 456 7890|123 45 6789| +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfebe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "|employee_id|        ssn|    phone_number|area_code|last4_phone_number|last4_ssn|\n",
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "|          1|123 45 6789| +1 123 456 7890|      123|              7890|     6789|\n",
      "|          2|456 78 9123|+91 234 567 8901|      234|              8901|     9123|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"ssn\", \"phone_number\") \\\n",
    "            .withColumn(\"area_code\", split(col(\"phone_number\"), \" \")[1].cast(\"int\")) \\\n",
    "            .withColumn(\"last4_phone_number\", split(col(\"phone_number\"), \" \")[3].cast(\"int\")) \\\n",
    "            .withColumn(\"last4_ssn\", split(col(\"ssn\"), \" \")[2].cast(\"int\")) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d46d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|count|\n",
      "+-----------+-----+\n",
      "|          1|    1|\n",
      "|          3|    8|\n",
      "|          4|    8|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF \\\n",
    "    .groupBy('employee_id') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8737842",
   "metadata": {},
   "source": [
    "### String manipulation functions - 2<a id=\"sixth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db4fde",
   "metadata": {},
   "source": [
    "#### String Manipulation - Padding\n",
    "Let us understand how to pad characters at the beginning or at the end of strings.\n",
    "* We typically pad characters to build fixed length values or records.\n",
    "* Fixed length values or records are extensively used in Mainframes based systems.\n",
    "* Length of each and every field in fixed length records is predetermined and if the value of the field is less than the predetermined length then we pad with a standard character.\n",
    "* In terms of numeric fields we pad with zero on the leading or left side. For non numeric fields, we pad with some standard character on leading or trailing side.\n",
    "* We use `lpad` to pad a string with a specific character on leading or left side and `rpad` to pad on trailing or right side.\n",
    "* Both lpad and rpad, take 3 arguments - column or expression, desired length and the character need to be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f9d04",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform simple tasks to understand the syntax of `lpad` or `rpad`.\n",
    "* Create a Dataframe with single value and single column.\n",
    "* Apply `lpad` to pad with - to Hello to make it 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e009b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99103fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dummy|\n",
      "+----------+\n",
      "|-----hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit('hello'), 10, '-').alias('dummy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c22fd",
   "metadata": {},
   "source": [
    "* Letâ€™s take the **employees** Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c52f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529af43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees). \\\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4235fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa534e9",
   "metadata": {},
   "source": [
    "* Use **pad** functions to convert each of the field into fixed length and concatenate. Here are the details for each of the fields.\n",
    " * Length of the employee_id should be 5 characters and should be padded with zero.\n",
    " * Length of first_name and last_name should be 10 characters and should be padded with - on the right side.\n",
    " * Length of salary should be 10 characters and should be padded with zero.\n",
    " * Length of the nationality should be 15 characters and should be padded with - on the right side.\n",
    " * Length of the phone_number should be 17 characters and should be padded with - on the right side.\n",
    " * Length of the ssn can be left as is. It is 11 characters.\n",
    " \n",
    "* Create a new Dataframe **empFixedDF** with column name **employee**. Preview the data by disabling truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cc0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|employee                                                                 |\n",
      "+-------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----1000.united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------1250.India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----1500.AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\n",
    "                concat(\n",
    "                    lpad(col('employee_id'), 5, '0'),\n",
    "                    rpad(col('first_name'), 10, '-'),\n",
    "                    rpad(col('last_name'), 10, '-'),\n",
    "                    lpad(col('salary'), 5, '0'),\n",
    "                    rpad(col('nationality'), 15, '-'),\n",
    "                    rpad(col('phone_number'), 17, '-'),\n",
    "                    'ssn').alias('employee')\n",
    "            ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c44bc",
   "metadata": {},
   "source": [
    "#### String Manipulation - Trimming\n",
    "Let us understand how to trim unwanted leading and trailing characters around a string.\n",
    "* We typically use trimming to remove unnecessary characters from fixed length records.\n",
    "* Fixed length records are extensively used in Mainframes and we might have to process it using Spark.\n",
    "* As part of processing we might want to remove leading or trailing characters such as 0 in case of numeric types and space or some standard character in case of alphanumeric types.\n",
    "* As of now Spark trim functions take the column as argument and remove leading or trailing spaces.\n",
    "* Trim spaces towards left - `ltrim`\n",
    "* Trim spaces towards right - `rtrim`\n",
    "* Trim spaces on both sides - `trim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2017851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         dummy|\n",
      "+--------------+\n",
      "|   Hello,     |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('   Hello,     ',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae4e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+------+\n",
      "|         dummy|      ltrim|    rtrim|  trim|\n",
      "+--------------+-----------+---------+------+\n",
      "|   Hello,     |Hello,     |   Hello,|Hello,|\n",
      "+--------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pyspark trim only remove spaces while sql style trim functions can trim any character\n",
    "df.withColumn('ltrim', ltrim(col('dummy'))) \\\n",
    "    .withColumn('rtrim', rtrim(col('dummy'))) \\\n",
    "    .withColumn('trim', trim(col('dummy'))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48c4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+------+\n",
      "|         dummy|      ltrim|   rtrim|  trim|\n",
      "+--------------+-----------+--------+------+\n",
      "|   Hello,     |Hello,     |   Hello|Hello,|\n",
      "+--------------+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For using sql style trim functions, use expr  \n",
    "df.withColumn('ltrim', expr(\"ltrim(dummy)\")) \\\n",
    "    .withColumn('rtrim', expr(\"rtrim(',', rtrim(dummy))\")) \\\n",
    "    .withColumn('trim', expr(\"trim(dummy)\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73dc164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|function_desc                                                                |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Function: rtrim                                                              |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrimRight             |\n",
      "|Usage: \n",
      "    rtrim(str) - Removes the trailing space characters from `str`.\n",
      "  |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION rtrim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0c9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \n",
      "    trim(str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(LEADING FROM str) - Removes the leading space characters from `str`.\n",
      "\n",
      "    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n",
      "\n",
      "    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\n",
      "\n",
      "    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\n",
      "  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION trim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23747013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+------+\n",
      "|         dummy|      ltrim|   rtrim|  trim|\n",
      "+--------------+-----------+--------+------+\n",
      "|   Hello,     |Hello,     |   Hello|Hello,|\n",
      "+--------------+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ltrim', expr(\"trim(LEADING FROM dummy)\")) \\\n",
    "    .withColumn('rtrim', expr(\"trim(TRAILING ',' FROM rtrim(dummy))\")) \\\n",
    "    .withColumn('trim', expr(\"trim(BOTH FROM dummy)\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd1ed7",
   "metadata": {},
   "source": [
    "### Date and Time overview<a id=\"seventh\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bedd31",
   "metadata": {},
   "source": [
    "* We can use `current_date` to get todayâ€™s server date. \n",
    " * Date will be returned using **yyyy-MM-dd** format.\n",
    "* We can use `current_timestamp` to get current server time. \n",
    " * Timestamp will be returned using **yyyy-MM-dd HH:mm:ss:SSS** format.\n",
    " * Hours will be by default in 24 hour format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b0b3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d31c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-01-29|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90592e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp()       |\n",
      "+--------------------------+\n",
      "|2022-01-29 11:45:49.100927|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d6fd01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-12-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20211201'), 'yyyyMMdd').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "273bc128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-01-21|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DDD represent day of the year\n",
    "df.select(to_date(lit('2021021'), 'yyyyDDD').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d50ecee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMM represent short form of month in string format\n",
    "df.select(to_date(lit('02-Mar-2021'), 'dd-MMM-yyyy').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0a9c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-03-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MMMM represent full form of month in string format\n",
    "df.select(to_date(lit('02-March-2021'), 'dd-MMMM-yyyy').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84e4d2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2021-12-01 17:25:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20211201 1725'), 'yyyyMMdd HHmm').alias('timestamp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0483e",
   "metadata": {},
   "source": [
    "### Date and Time arithmetic<a id=\"eigth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bdaaf",
   "metadata": {},
   "source": [
    "Let us perform Date and Time Arithmetic using relevant functions.\n",
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting a number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e58ad2",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform some tasks related to date arithmetic.\n",
    "* Get help on each and every function first and understand what all arguments need to be passed.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdc1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4d767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, \"date STRING, time STRING\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75542a9",
   "metadata": {},
   "source": [
    "* Add 10 days to both date and time values.\n",
    "* Subtract 10 days from both date and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80da9170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a2cb11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|      date|                time|date_add_date|date_add_time|date_sub_date|date_sub_time|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|   2014-03-10|   2014-02-18|   2014-02-18|   2014-02-18|\n",
      "|2016-02-29|2016-02-29 08:08:...|   2016-03-10|   2016-02-19|   2016-02-19|   2016-02-19|\n",
      "|2017-10-31|2017-12-31 11:59:...|   2017-11-10|   2017-12-21|   2017-10-21|   2017-12-21|\n",
      "|2019-11-30|2019-08-31 00:00:...|   2019-12-10|   2019-08-21|   2019-11-20|   2019-08-21|\n",
      "+----------+--------------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.withColumn('date_add_date', date_add(col('date'), 10)) \\\n",
    "    .withColumn('date_add_time', date_sub(col('time'), 10)) \\\n",
    "    .withColumn('date_sub_date', date_sub(col('date'), 10)) \\\n",
    "    .withColumn('date_sub_time', date_sub(col('time'), 10)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc034ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+----------+\n",
      "|      date|                time|  date_add|  date_sub|\n",
      "+----------+--------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:...|2014-02-18|2014-03-10|\n",
      "|2016-02-29|2016-02-29 08:08:...|2016-02-19|2016-03-10|\n",
      "|2017-10-31|2017-12-31 11:59:...|2017-10-21|2017-11-10|\n",
      "|2019-11-30|2019-08-31 00:00:...|2019-11-20|2019-12-10|\n",
      "+----------+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you pass negative number to date_add, it will subtract\n",
    "# If you pass negative number to date_sub, it will add\n",
    "datetimesDF.withColumn('date_add', date_add(col('date'), -10)) \\\n",
    "        .withColumn('date_sub', date_sub(col('date'), -10)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39274c57",
   "metadata": {},
   "source": [
    "* Get the difference between current_date and date values as well as current_timestamp and time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae84eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function datediff in module pyspark.sql.functions:\n",
      "\n",
      "datediff(end, start)\n",
      "    Returns the number of days from `start` to `end`.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "    [Row(diff=32)]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "436c9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+-------------+\n",
      "|      date|                time|datediff_date|datediff_time|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|         2892|         2892|\n",
      "|2016-02-29|2016-02-29 08:08:...|         2161|         2161|\n",
      "|2017-10-31|2017-12-31 11:59:...|         1551|         1490|\n",
      "|2019-11-30|2019-08-31 00:00:...|          791|          882|\n",
      "+----------+--------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF \\\n",
    "    .withColumn(\"datediff_date\", datediff(current_date(), col('date'))) \\\n",
    "    .withColumn(\"datediff_time\", datediff(current_timestamp(), col('time'))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812dc6d2",
   "metadata": {},
   "source": [
    "* Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "* Add 3 months to both date values as well as time values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45430ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+-------------------+---------------+---------------+\n",
      "|      date|                time|months_between_dates|months_between_time|add_months_date|add_months_time|\n",
      "+----------+--------------------+--------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:...|               95.03|              95.04|     2014-05-28|     2014-05-28|\n",
      "|2016-02-29|2016-02-29 08:08:...|                71.0|               71.0|     2016-05-29|     2016-05-29|\n",
      "|2017-10-31|2017-12-31 11:59:...|               50.94|              48.94|     2018-01-31|     2018-03-31|\n",
      "|2019-11-30|2019-08-31 00:00:...|               25.97|              28.96|     2020-02-29|     2019-11-30|\n",
      "+----------+--------------------+--------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF \\\n",
    "    .withColumn(\"months_between_dates\", round(months_between(current_date(), 'date'), 2)) \\\n",
    "    .withColumn(\"months_between_time\", round(months_between(current_timestamp(), 'time'), 2)) \\\n",
    "    .withColumn(\"add_months_date\", add_months('date', 3)) \\\n",
    "    .withColumn(\"add_months_time\", add_months('time', 3)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5954d8",
   "metadata": {},
   "source": [
    "### Date and Time - trunc and date_trunc<a id=\"ninth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5cdc8",
   "metadata": {},
   "source": [
    "In Data Warehousing we quite often run to date reports such as week to date, month to date, year to date etc.\n",
    "* We can use `trunc` or `date_trunc` for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n",
    "* We can use `trunc` to get beginning date of the month or year by passing date or timestamp to it - for example `trunc(current_date(), \"MM\")` will give the first of the current month.\n",
    "* We can use `date_trunc` to get beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n",
    " * Get beginning date based on month - `date_trunc(\"MM\", current_timestamp())`\n",
    " * Get beginning time based on day - `date_trunc(\"DAY\", current_timestamp())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575780cc",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87cd011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]\n",
    "datetimesDF = spark.createDataFrame(datetimes, schema=\"date STRING, time STRING\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661aa893",
   "metadata": {},
   "source": [
    "* Get beginning month date using date field and beginning year date using time field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c637bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function trunc in module pyspark.sql.functions:\n",
      "\n",
      "trunc(date, format)\n",
      "    Returns date truncated to the unit specified by the format.\n",
      "    \n",
      "    :param format: 'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "    >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "    [Row(year=datetime.date(1997, 1, 1))]\n",
      "    >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "    [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55dd9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_trunc in module pyspark.sql.functions:\n",
      "\n",
      "date_trunc(format, timestamp)\n",
      "    Returns timestamp truncated to the unit specified by the format.\n",
      "    \n",
      "    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\n",
      "        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    .. versionadded:: 2.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0fe8dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |datetrunc |timetrunc |\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF \\\n",
    "    .withColumn(\"datetrunc\", trunc('date', 'MM')) \\\n",
    "    .withColumn(\"timetrunc\", trunc('time', 'YY')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fbb9f7",
   "metadata": {},
   "source": [
    "* Get beginning hour time using date and time field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62bd2fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |datetrunc          |timetrunc          |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF \\\n",
    "    .withColumn(\"datetrunc\", date_trunc('HOUR', 'date')) \\\n",
    "    .withColumn(\"timetrunc\", date_trunc('HOUR', 'time')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09269e5f",
   "metadata": {},
   "source": [
    "### Date and Time - extracting information<a id=\"tenth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419a3a1",
   "metadata": {},
   "source": [
    "Overview about Date and Time extract functions. Here are the extract functions that are useful which are self explanatory.\n",
    "\n",
    "* `year`\n",
    "* `month`\n",
    "* `weekofyear`\n",
    "* `dayofyear`\n",
    "* `dayofmonth`\n",
    "* `dayofweek`\n",
    "* `hour`\n",
    "* `minute`\n",
    "* `second`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f558e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b761622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "|current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|hour|minute|second|\n",
      "+------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "|  2022-01-29|2022|    1|         4|       29|        29|        7|   0|     0|     0|\n",
      "+------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_date().alias('current_date'))  \\\n",
    "    .withColumn('year', year(current_date())) \\\n",
    "    .withColumn('month', month(current_date())) \\\n",
    "    .withColumn('weekofyear', weekofyear(current_date())) \\\n",
    "    .withColumn('dayofyear', dayofyear(current_date())) \\\n",
    "    .withColumn('dayofmonth', dayofmonth(current_date())) \\\n",
    "    .withColumn('dayofweek', dayofweek(current_date())) \\\n",
    "    .withColumn('hour', hour(current_date())) \\\n",
    "    .withColumn('minute', minute(current_date())) \\\n",
    "    .withColumn('second', second(current_date())) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6943671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "|        current_date|year|month|weekofyear|dayofyear|dayofmonth|dayofweek|hour|minute|second|\n",
      "+--------------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "|2022-01-29 19:08:...|2022|    1|         4|       29|        29|        7|  19|     8|    20|\n",
      "+--------------------+----+-----+----------+---------+----------+---------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    current_timestamp().alias('current_date'))  \\\n",
    "    .withColumn('year', year(current_timestamp())) \\\n",
    "    .withColumn('month', month(current_timestamp())) \\\n",
    "    .withColumn('weekofyear', weekofyear(current_timestamp())) \\\n",
    "    .withColumn('dayofyear', dayofyear(current_timestamp())) \\\n",
    "    .withColumn('dayofmonth', dayofmonth(current_timestamp())) \\\n",
    "    .withColumn('dayofweek', dayofweek(current_timestamp())) \\\n",
    "    .withColumn('hour', hour(current_timestamp())) \\\n",
    "    .withColumn('minute', minute(current_timestamp())) \\\n",
    "    .withColumn('second', second(current_timestamp())) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82512f7b",
   "metadata": {},
   "source": [
    "### Dealing with UNIX timestamp<a id=\"eleventh\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8aa3e",
   "metadata": {},
   "source": [
    "Let us understand how to deal with Unix Timestamp in Spark.\n",
    "\n",
    "* It is an integer and started from January 1st 1970 Midnight UTC.\n",
    "* Beginning time is also known as epoch and is incremented by 1 every second.\n",
    "* We can convert Unix Timestamp to regular date or timestamp and vice versa.\n",
    "* We can use `unix_timestamp` to convert regular date or timestamp to a unix timestamp value. For example `unix_timestamp(lit(\"2019-11-19 00:00:00\"))`\n",
    "* We can use `from_unixtime` to convert unix timestamp to regular date or timestamp. For example `from_unixtime(lit(1574101800))`\n",
    "* We can also pass format to both the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8d4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04680533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+\n",
      "|dateid  |date      |time                   |\n",
      "+--------+----------+-----------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+--------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes).toDF(\"dateid\", \"date\", \"time\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd8121",
   "metadata": {},
   "source": [
    "* Get unix timestamp for dateid, date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dae7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dateid: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7163d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function unix_timestamp in module pyspark.sql.functions:\n",
      "\n",
      "unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "    Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "    to Unix time stamp (in seconds), using the default timezone and the default\n",
      "    locale, return null if fail.\n",
      "    \n",
      "    if `timestamp` is None, then it returns current timestamp.\n",
      "    \n",
      "    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "    >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "    [Row(unix_time=1428476400)]\n",
      "    >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(unix_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f548be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------+----------+----------+\n",
      "|  dateid|      date|                time|unix_date_id| unix_date| unix_time|\n",
      "+--------+----------+--------------------+------------+----------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:...|  1393525800|1393525800|1393561800|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:...|  1456684200|1456684200|1456713488|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:...|  1509388200|1509388200|1514701799|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:...|  1575052200|1575052200|1567189800|\n",
      "+--------+----------+--------------------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unix_timestamp accept string value\n",
    "datetimesDF. \\\n",
    "withColumn(\"unix_date_id\", unix_timestamp(col(\"dateid\").cast(\"string\"), 'yyyyMMdd')). \\\n",
    "withColumn(\"unix_date\", unix_timestamp(\"date\", \"yyyy-MM-dd\")). \\\n",
    "withColumn(\"unix_time\", unix_timestamp(\"time\", \"yyyy-MM-dd HH:mm:ss.SSS\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a74b32",
   "metadata": {},
   "source": [
    "* Create a dataframe by name unixtimesDF with one column unixtime using 4 values. You can use the unix timestamp generated for time column in previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f81e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------------+---------+----------+\n",
      "|  dateid|      date|                time|unix_date_id|unix_date| unix_time|\n",
      "+--------+----------+--------------------+------------+---------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:...|  1393525800|     null|1393561800|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:...|  1456684200|     null|1456713488|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:...|  1509388200|     null|1514701799|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:...|  1575052200|     null|1567189800|\n",
      "+--------+----------+--------------------+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unix_timestamp accepts string of format yyyy-MM-dd HH:mm:ss by default\n",
    "# If you do not pass the format, it will return null \n",
    "datetimesDF. \\\n",
    "withColumn(\"unix_date_id\", unix_timestamp(col(\"dateid\").cast(\"string\"), 'yyyyMMdd')). \\\n",
    "withColumn(\"unix_date\", unix_timestamp(\"date\")). \\\n",
    "withColumn(\"unix_time\", unix_timestamp(\"time\", \"yyyy-MM-dd HH:mm:ss.SSS\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "053ededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unixtimes = [\n",
    "(1393561800, ),\n",
    "(1456713488, ),\n",
    "(1514701799, ),\n",
    "(1567189800, )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8180064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393561800|\n",
      "|1456713488|\n",
      "|1514701799|\n",
      "|1567189800|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF = spark.createDataFrame(unixtimes).toDF(\"unixtime\")\n",
    "unixtimesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28e96ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unixtime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b181447d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+\n",
      "|  unixtime|    Date|               time|\n",
      "+----------+--------+-------------------+\n",
      "|1393561800|20140228|2014-02-28 10:00:00|\n",
      "|1456713488|20160229|2016-02-29 08:08:08|\n",
      "|1514701799|20171231|2017-12-31 11:59:59|\n",
      "|1567189800|20190831|2019-08-31 00:00:00|\n",
      "+----------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF. \\\n",
    "withColumn(\"Date\", from_unixtime(\"unixtime\", \"yyyyMMdd\")). \\\n",
    "withColumn(\"time\", from_unixtime(\"unixtime\")) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaba35",
   "metadata": {},
   "source": [
    "### Dealing with Nulls<a id=\"twelth\"></a>\n",
    "\n",
    "* We can use `coalesce` to return first non null value.\n",
    "* We also have traditional SQL style functions such as `nvl`. However, they can be used either with `expr` or `selectExpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2bb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 10,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c69bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function coalesce in module pyspark.sql.functions:\n",
      "\n",
      "coalesce(*cols)\n",
      "    Returns the first column that is not null.\n",
      "    \n",
      "    >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "    >>> cDf.show()\n",
      "    +----+----+\n",
      "    |   a|   b|\n",
      "    +----+----+\n",
      "    |null|null|\n",
      "    |   1|null|\n",
      "    |null|   2|\n",
      "    +----+----+\n",
      "    \n",
      "    >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "    +--------------+\n",
      "    |coalesce(a, b)|\n",
      "    +--------------+\n",
      "    |          null|\n",
      "    |             1|\n",
      "    |             2|\n",
      "    +--------------+\n",
      "    \n",
      "    >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "    +----+----+----------------+\n",
      "    |   a|   b|coalesce(a, 0.0)|\n",
      "    +----+----+----------------+\n",
      "    |null|null|             0.0|\n",
      "    |   1|null|             1.0|\n",
      "    |null|   2|             0.0|\n",
      "    +----+----+----------------+\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(coalesce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28783cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|nulls|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|   10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|    0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|     |\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|   10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "withColumn('nulls', coalesce('bonus', lit(0))). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b996f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|nulls|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|   10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|    0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|    0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|   10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the empty string to int, it will return null\n",
    "employeesDF. \\\n",
    "withColumn('nulls', coalesce(col('bonus').cast('int'), lit(0))). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "081b3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|payment|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789| 1000.1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123| 1250.0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|  750.0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118| 1500.1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find total payment including salary and bonus\n",
    "employeesDF. \\\n",
    "withColumn('payment', col('salary') + (coalesce(col('bonus').cast('int'), lit(0)) / 100)). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976e097d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|nulls|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|   10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|    0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|     |\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|   10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL like statement for handling nulls or empties\n",
    "employeesDF. \\\n",
    "withColumn('nulls', expr(\"nvl(bonus, 0)\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b15d94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|nulls|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|   10|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|    0|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|    0|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|   10|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null with empty string then fill 0\n",
    "employeesDF. \\\n",
    "withColumn('nulls', expr(\"nvl(nullif(bonus, ''), 0)\")). \\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b920c0",
   "metadata": {},
   "source": [
    "### Conclusion<a id=\"thirteen\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88273da9",
   "metadata": {},
   "source": [
    "### Using CASE and WHEN\n",
    "\n",
    "* `CASE` and `WHEN` is typically used to apply transformations based on conditions. We can use `CASE` and `WHEN` similar to SQL using `expr` or `selectExpr`.\n",
    "* If we want to use APIs, Spark provides functions such as `when` and `otherwise`. `when` is available as part of `pyspark.sql.functions`. On top of column type that is generated using `when` we should be able to invoke `otherwise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f703abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.withColumn(\"bonus\", \n",
    "                      expr(\"\"\" \n",
    "                      CASE WHEN bonus is NULL OR bonus='' THEN 0 ELSE bonus end\n",
    "                      \"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c076e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65cc69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|    0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|    0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|   10|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.withColumn(\"bonus\", \n",
    "                      when((col('bonus').isNull()) | (col('bonus') == lit('')), 0).otherwise(col('bonus'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefc2eb",
   "metadata": {},
   "source": [
    "* Create a dataframe using list called as persons and categorize them based up on following rules.\n",
    "\n",
    "| Age Range | Category |\n",
    "| ---       |      --- |\n",
    "| 0 to 2 Months| New Born |\n",
    "| 2+ to 12 Months| Infant |\n",
    "| 12+ to 48 Months| Toddler |\n",
    "| 48+ to 144 Months| Kids |\n",
    "| 144+ Months| Teenager or Adult |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "653b73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [\n",
    "    (1, 1),\n",
    "    (2, 13),\n",
    "    (3, 18),\n",
    "    (4, 60),\n",
    "    (5, 120),\n",
    "    (6, 0),\n",
    "    (7, 12),\n",
    "    (8, 160)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcd32f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2| 13|\n",
      "|  3| 18|\n",
      "|  4| 60|\n",
      "|  5|120|\n",
      "|  6|  0|\n",
      "|  7| 12|\n",
      "|  8|160|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF = spark.createDataFrame(persons, schema='id INT, age INT')\n",
    "personsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07e6b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "| id|age|         category|\n",
      "+---+---+-----------------+\n",
      "|  1|  1|         New Born|\n",
      "|  2| 13|          Toddler|\n",
      "|  3| 18|          Toddler|\n",
      "|  4| 60|              Kid|\n",
      "|  5|120|              Kid|\n",
      "|  6|  0|         New Born|\n",
      "|  7| 12|           Infant|\n",
      "|  8|160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "withColumn('category',\n",
    "          expr(\"\"\"\n",
    "          CASE WHEN age BETWEEN 0 AND 2 THEN 'New Born'\n",
    "               WHEN age > 2 AND age <= 12 THEN 'Infant'\n",
    "               WHEN age > 12 AND age <= 48 THEN 'Toddler'\n",
    "               WHEN age > 48 AND age <= 144 THEN 'Kid'\n",
    "               ELSE 'Teenager or Adult'\n",
    "               END\n",
    "          \"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600323fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "| id|age|         category|\n",
      "+---+---+-----------------+\n",
      "|  1|  1|         New Born|\n",
      "|  2| 13|          Toddler|\n",
      "|  3| 18|          Toddler|\n",
      "|  4| 60|              Kid|\n",
      "|  5|120|              Kid|\n",
      "|  6|  0|         New Born|\n",
      "|  7| 12|           Infant|\n",
      "|  8|160|Teenager or Adult|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personsDF. \\\n",
    "withColumn('category',\n",
    "               when(col('age').between(0, 2), 'New Born').\n",
    "               when((col('age') > 2) & (col('age') <= 12), 'Infant').\n",
    "               when((col('age') > 12) & (col('age') <= 48), 'Toddler').\n",
    "               when((col('age') > 48) & (col('age') <= 144), 'Kid').\n",
    "               otherwise('Teenager or Adult')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
