{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452882c5",
   "metadata": {},
   "source": [
    "## Read data from Spark DataFrame into files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9fb78",
   "metadata": {},
   "source": [
    "* Reading files using direct APIs such as `csv, json`, etc under `spark.read`.\n",
    "* Reading files using `format` and load under `spark.read`.\n",
    "* Specifying options as arguments as well as using functions such as `option` and `options`.\n",
    "* Supported file formats:\n",
    "    * `csv`\n",
    "    * `text`\n",
    "    * `json`\n",
    "    * `parquet`\n",
    "    * `orc`\n",
    "* Other common file formats:\n",
    "    * `xml`\n",
    "    * `avro`\n",
    "* Important file formats for certification - `csv`, `json`, `parquet`\n",
    "* Reading compressed files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a496ce",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "* Check if the files are compressed(gz, snappy, bz2, etc). Most common ones are gz and snappy.\n",
    "* Understand the file format(text, json, avro, parquet etc). Sometimes files will not have extensions.\n",
    "* If files does not have extensions, make sure to confirm the details by going through the tech spec or by opening the file.\n",
    "* We will get the tech specs from our leads or architects while working on real world projects.\n",
    "* If the files are of text file format, check if the data is delimited or separated by a specific character.\n",
    "* Use appropriate API under `spark.read` to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b88bbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d85c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName('ReadDataSparkDF'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc95be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd45f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:00|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:00|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:00|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:00|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:00|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:00|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:00|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:00|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:00|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:00|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:00|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:00|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:00|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:00|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:00|             9198|     PROCESSING|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.schema(schema).csv('../data/orders.csv', header=None)\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3993ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9debd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ca4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_json = spark.read \\\n",
    "        .json('../data/orders.json', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377a8e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fe333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99988e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|            11599|   00:00.0|       1|         CLOSED|\n",
      "|              256|   00:00.0|       2|PENDING_PAYMENT|\n",
      "|            12111|   00:00.0|       3|       COMPLETE|\n",
      "|             8827|   00:00.0|       4|         CLOSED|\n",
      "|            11318|   00:00.0|       5|       COMPLETE|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_json.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0886c9c7",
   "metadata": {},
   "source": [
    "#### JSON to PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1199d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/retail_db_json/'\n",
    "output_dir = '../data/retail_db_parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869ae8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536ec1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/categories` from json to parquet\n",
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/customers` from json to parquet\n",
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/departments` from json to parquet\n",
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/orders` from json to parquet\n",
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/order_items` from json to parquet\n",
      "Converting data in `file:/E:/Practice/PySpark/data/retail_db_json/products` from json to parquet\n"
     ]
    }
   ],
   "source": [
    "# Get all the files from input_path (json) and store it in another location (parquet)\n",
    "for file in list_status:\n",
    "    file_path = file.getPath() \n",
    "    if not file_path.getName().endswith('sql'):\n",
    "        print(f\"Converting data in `{file_path}` from json to parquet\")\n",
    "        df = spark.read.json(str(file_path))\n",
    "        df.coalesce(1).write.parquet(f'{output_dir}/{file_path.getName()}', mode='overwrite') # coalesce -> create 1 copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9926829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.parquet(f'{output_dir}/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99df881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81046028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3e3cf",
   "metadata": {},
   "source": [
    "#### Databricks Code for converting json files to parquet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80a170ab",
   "metadata": {},
   "source": [
    "# Get the files from directory\n",
    "dbutils.fs.ls(input_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab5f76d0",
   "metadata": {},
   "source": [
    "# Get all the files from input_path (json) and store it in another location (parquet)\n",
    "for file_details in dbutils.fs.ls(input_dir):\n",
    "    if not file_details.path.endswith('sql'):\n",
    "        data_set_dir = file_details.path.split('/')[-2] # Get name of the file\n",
    "        print(f\"Converting data in `{file_details.path}` from json to parquet\")\n",
    "        df = spark.read.json(file_details.path)\n",
    "        df.coalesce(1).write.parquet(f'{output_dir}/{data_set_dir}', mode='overwrite') # coalesce -> create 1 copy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebd6f99b",
   "metadata": {},
   "source": [
    "dbutils.fs.ls(f'{output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880765f",
   "metadata": {},
   "source": [
    "#### Convert COMMA Separated to PIPE Separated Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97378fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/retail_db/'\n",
    "output_dir = '../data/retail_db_pipe/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "895279bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24642106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data in `../data/retail_db/` from csv to pipe separated\n",
      "Converting data in `../data/retail_db/` from csv to pipe separated\n",
      "Converting data in `../data/retail_db/` from csv to pipe separated\n",
      "Converting data in `../data/retail_db/` from csv to pipe separated\n",
      "Converting data in `../data/retail_db/` from csv to pipe separated\n",
      "Converting data in `../data/retail_db/` from csv to pipe separated\n"
     ]
    }
   ],
   "source": [
    "for file in list_status:\n",
    "    file_path = file.getPath()\n",
    "    file_name = file_path.getName()\n",
    "    if not file_name.endswith('sql'):\n",
    "        print(f\"Converting data in `{input_dir}` from csv to pipe separated\")\n",
    "        df = spark.read.csv(str(file_path))\n",
    "        df.coalesce(1).write.mode('overwrite').csv(f'{output_dir}/{file_name}', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c812d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c29522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.schema(schema).csv(f'{output_dir}/orders', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8862da60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd439723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df97dc",
   "metadata": {},
   "source": [
    "#### Reading data from CSV files into Spark DataFrame using multiple approaches.\n",
    "\n",
    "* `spark.read.csv('path_to_folder')`\n",
    "* `spark.read.format('csv').load('path_to_folder')`\n",
    "* We can explicitely specify the schema as `string` or using `StructType`.\n",
    "* We can also read the data which is delimited or separated by other characters than comma.\n",
    "* If the files have header we can create the dataframe with schema by using options as `header` and `inferSchema`. It will pick column names from the header while data types will be inferred based on the data.\n",
    "* If the files does not have header we can create the dataframe with schema by passing column names using `toDF` and by using `inferSchema` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29483544",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('../data/orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d85240a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68bb2ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6448cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "823befb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('../data/orders.csv').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95164c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/orders.csv', schema=schema).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fb550e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('order_id', IntegerType()),\n",
    "    StructField('order_date', TimestampType()),\n",
    "    StructField('order_customer_id', IntegerType()),\n",
    "    StructField('order_status', StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2234d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------------+---------------+\n",
      "|order_id|         order_date|order_customer_id|   order_status|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:00|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:00|            11318|       COMPLETE|\n",
      "+--------+-------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.schema(schema).csv('../data/orders.csv').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd30a047",
   "metadata": {},
   "source": [
    "#### Using toDF and inferSchema using csv to create spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b40c41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'), ('_c1', 'string'), ('_c2', 'int'), ('_c3', 'string')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('inferSchema', True).csv('../data/orders.csv').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5d68862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method toDF in module pyspark.sql.dataframe:\n",
      "\n",
      "toDF(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` that with new specified column names\n",
      "    \n",
      "    :param cols: list of new column names (string)\n",
      "    \n",
      "    >>> df.toDF('f1', 'f2').collect()\n",
      "    [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.option('inferSchema', True).csv('../data/orders.csv').toDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58a17719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'string'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('inferSchema', True).csv('../data/orders.csv').toDF('order_id',\n",
    "'order_date',\n",
    "'order_customer_id',\n",
    "'order_status').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f65f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['order_id',\n",
    "'order_date',\n",
    "'order_customer_id',\n",
    "'order_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2529155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'string'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('inferSchema', True).csv('../data/orders.csv').toDF(*columns).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec9ae8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'string'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('../data/orders.csv', inferSchema=True).toDF(*columns).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ff58d",
   "metadata": {},
   "source": [
    " #### Using options using different ways while creating the dataframe\n",
    " \n",
    " * Using keyword arguments as part of APIs. We can use keyword arguments as part of `load` as well as direct API(`csv`).\n",
    " * `spark.read.option`\n",
    " * `spark.read.options`\n",
    " * If key the option is incorrect then the options will be ignored.\n",
    " \n",
    " Depending up on the API based on the file format the options as well as arguments way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b21389af",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('../data/retail_db_pipe/orders', sep='|', header=None, inferSchema=True).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d062f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0efc85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.format('csv').load('../data/retail_db_pipe/orders', sep='|', header=None, inferSchema=True).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1308995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d1f18fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('sep', '|').option('header', None).option('inferSchema', True). \\\n",
    "csv('../data/retail_db_pipe/orders').toDF(*columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05c08d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.options(sep='|', header=None, inferSchema=True). \\\n",
    "csv('../data/retail_db_pipe/orders').toDF(*columns).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07545e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'sep': '|',\n",
    "    'header': None,\n",
    "    'inferSchema': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7b7dddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.options(**options). \\\n",
    "csv('../data/retail_db_pipe/orders').toDF(*columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e2f49",
   "metadata": {},
   "source": [
    "#### Reading JSON files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c30bbe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|            11599|   00:00.0|       1|         CLOSED|\n",
      "|              256|   00:00.0|       2|PENDING_PAYMENT|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../data/orders.json', multiLine=True).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ffb3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load('../data/retail_db_json/orders', multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e16d95",
   "metadata": {},
   "source": [
    "df.inputFiles() # Not working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69326ec",
   "metadata": {},
   "source": [
    "* If inferSchema is used, entire data need to be read to infer the schema accurately while creating the dataframe.\n",
    "* If the data size is too big then additional time will be spent to infer the schema.\n",
    "* When we explicitly specify the schema, data will not be read while creating the dataframe.\n",
    "* As we have seen we should be able to explicitly specify the schema using string or StructType.\n",
    "* Inferring schmea will come handy to quickly understand the structure of the data as part of proof of concepts as well as design.\n",
    "* Schema will be inferred by default for files of type JSON, Parquet and ORC. Column names and datatypes will be inferred using metadata that will be associated with these types of files.\n",
    "* Inferring the schema on CSV files will create dataframes with system generated column names. If inferSchema is used, then the dataframe will determine the datatypes. if the files contain header, then column names can be inherited using it. If not, we need to explicitly pass the columns using `toDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "304358f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('../data/retail_db_parquet/orders').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b9b3f394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/retail_db_parquet/orders').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fdf409fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "    order_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f354d189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.schema(schema).parquet('../data/retail_db_parquet/orders').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8da8f9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('parquet').load('../data/retail_db_parquet/orders', schema=schema).dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
