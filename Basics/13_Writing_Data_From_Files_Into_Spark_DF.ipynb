{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d048bf7",
   "metadata": {},
   "source": [
    "## Writing data from files into spark dataframe\n",
    "\n",
    "* Writing files using direct APIs such as `csv, json`, etc under `df.write` where df is of type Spark's DataFrameWriter.\n",
    "* Writing files using `format` and `save` under `df.write`.\n",
    "* Specifying options as arguments as well as using functions such as `option` and `options`.\n",
    "* Supported file formats:\n",
    "    * `csv`\n",
    "    * `text`\n",
    "    * `json`\n",
    "    * `parquet`\n",
    "    * `orc`\n",
    "* Other common file formats:\n",
    "    * `xml`\n",
    "    * `avro`\n",
    "* Important file formats for certification - `csv`, `text`, `json`, `parquet`\n",
    "* Writing into compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a6ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a813ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "userName = 'CodeInDNA'\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(f'{userName} - JoinSparkDF'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94523452",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = [\n",
    "    {\n",
    "        'course_id': 1,\n",
    "        'course_title': 'Mastering Python',\n",
    "        'course_published_dt': datetime.date(2021, 1, 14),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 2, 18, 16, 57, 25)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 2,\n",
    "        'course_title': 'Data Engineering Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 2, 10),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 5, 12, 7, 33)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 3,\n",
    "        'course_title': 'Mastering PySpark',\n",
    "        'course_published_dt': datetime.date(2021, 1, 7),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 6, 10, 5, 42)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 4,\n",
    "        'course_title': 'AWS Essentials',\n",
    "        'course_published_dt': datetime.date(2021, 3, 19),\n",
    "        'is_active': False,\n",
    "        'last_updated_ts': datetime.datetime(2021, 4, 10, 2, 25, 36)\n",
    "    },\n",
    "    {\n",
    "        'course_id': 5,\n",
    "        'course_title': 'Docker 101',\n",
    "        'course_published_dt': datetime.date(2021, 2, 28),\n",
    "        'is_active': True,\n",
    "        'last_updated_ts': datetime.datetime(2021, 3, 21, 7, 18, 52)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3eba995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "|        3|   Mastering PySpark|         2021-01-07|     true|2021-04-06 10:05:42|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10 02:25:36|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21 07:18:52|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df = spark.createDataFrame([Row(**course) for course in courses])\n",
    "\n",
    "courses_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4f3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.readwriter.DataFrameWriter"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(courses_df.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e93b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df.write.json('../data/courses', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146e435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df.write.format('json').save('../data/courses', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb571ca9",
   "metadata": {},
   "source": [
    "#### Steps to follow the spark dataframes into files\n",
    "\n",
    "* Make sure to analyse the schema of the dataframe.\n",
    "* Make sure you have write permissions on the target location.\n",
    "* Understand whether you want to overwrite or append or ignore or throw exception in case target folder already exists.\n",
    "* Decide whether you would like to compressed or not by default.\n",
    "* Make sure you understand whether the files will be compressed or not by default.\n",
    "* Use appropriate APIs along with right arguments based up on the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b470cc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('course_id', 'bigint'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'date'),\n",
       " ('is_active', 'boolean'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a093b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df.write.csv('../data/courses_csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729fa4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df.write.format('csv').save('../data/courses_csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d322821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|4,AWS Essentials,2021-03-19,false,2021-04-10T02:25:36.000+05:30            |\n",
      "|5,Docker 101,2021-02-28,true,2021-03-21T07:18:52.000+05:30                 |\n",
      "|2,Data Engineering Essentials,2021-02-10,true,2021-03-05T12:07:33.000+05:30|\n",
      "|3,Mastering PySpark,2021-01-07,true,2021-04-06T10:05:42.000+05:30          |\n",
      "|1,Mastering Python,2021-01-14,true,2021-02-18T16:57:25.000+05:30           |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('../data/courses_csv').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398630fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce: Create 1 file instead of many small files\n",
    "courses_df.coalesce(1).write.csv('../data/courses_csv', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a588a8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|value                                                                      |\n",
      "+---------------------------------------------------------------------------+\n",
      "|course_id,course_title,course_published_dt,is_active,last_updated_ts       |\n",
      "|1,Mastering Python,2021-01-14,true,2021-02-18T16:57:25.000+05:30           |\n",
      "|2,Data Engineering Essentials,2021-02-10,true,2021-03-05T12:07:33.000+05:30|\n",
      "|3,Mastering PySpark,2021-01-07,true,2021-04-06T10:05:42.000+05:30          |\n",
      "|4,AWS Essentials,2021-03-19,false,2021-04-10T02:25:36.000+05:30            |\n",
      "|5,Docker 101,2021-02-28,true,2021-03-21T07:18:52.000+05:30                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('../data/courses_csv/').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210a4edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|     last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18T16:57:...|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05T12:07:...|\n",
      "|        3|   Mastering PySpark|         2021-01-07|     true|2021-04-06T10:05:...|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10T02:25:...|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21T07:18:...|\n",
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/courses_csv/', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecceaa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      "            exists.\n",
      "    \n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      "                        snappy and deflate).\n",
      "    :param sep: sets a separator (one or more characters) for each field and value. If None is\n",
      "                set, it uses the default value, ``,``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``\n",
      "    :param escapeQuotes: a flag indicating whether values containing quotes should always\n",
      "                         be enclosed in quotes. If None is set, it uses the default value\n",
      "                         ``true``, escaping all values containing a quote character.\n",
      "    :param quoteAll: a flag indicating whether all values should always be enclosed in\n",
      "                      quotes. If None is set, it uses the default value ``false``,\n",
      "                      only escaping values containing a quote character.\n",
      "    :param header: writes the names of columns as the first line. If None is set, it uses\n",
      "                   the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats follow\n",
      "                       the formats at `datetime pattern`_.\n",
      "                       This applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format.\n",
      "                            Custom date formats follow the formats at `datetime pattern`_.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    :param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\n",
      "                                    values being written should be skipped. If None is set, it\n",
      "                                    uses the default value, ``true``.\n",
      "    :param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\n",
      "                                     values being written should be skipped. If None is set, it\n",
      "                                     uses the default value, ``true``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise..\n",
      "    :param encoding: sets the encoding (charset) of saved csv files. If None is set,\n",
      "                     the default UTF-8 charset will be used.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, ``\"\"``.\n",
      "    :param lineSep: defines the line separator that should be used for writing. If None is\n",
      "                    set, it uses the default value, ``\\\\n``. Maximum length is 1 character.\n",
      "    \n",
      "    >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.write.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccff1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressions: bzip2, gzip, snappy, lz4, deflate\n",
    "courses_df.coalesce(1).write.csv('../data/courses_csv', header=True, compression='gzip', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f3408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|course_id|course_title               |course_published_dt|is_active|last_updated_ts              |\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|1        |Mastering Python           |2021-01-14         |true     |2021-02-18T16:57:25.000+05:30|\n",
      "|2        |Data Engineering Essentials|2021-02-10         |true     |2021-03-05T12:07:33.000+05:30|\n",
      "|3        |Mastering PySpark          |2021-01-07         |true     |2021-04-06T10:05:42.000+05:30|\n",
      "|4        |AWS Essentials             |2021-03-19         |false    |2021-04-10T02:25:36.000+05:30|\n",
      "|5        |Docker 101                 |2021-02-28         |true     |2021-03-21T07:18:52.000+05:30|\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/courses_csv/', header=True).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0057c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|     last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18T16:57:...|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05T12:07:...|\n",
      "|        3|   Mastering PySpark|         2021-01-07|     true|2021-04-06T10:05:...|\n",
      "|        4|      AWS Essentials|         2021-03-19|    false|2021-04-10T02:25:...|\n",
      "|        5|          Docker 101|         2021-02-28|     true|2021-03-21T07:18:...|\n",
      "+---------+--------------------+-------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the data and replace the separator to '|'\n",
    "input_dir = '../data/courses_csv/'\n",
    "\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_dir))\n",
    "\n",
    "for file in list_status:\n",
    "    if file.getPath().getName() != '_SUCCESS':\n",
    "        df = spark.read.csv(str(file.getPath()), header=True)\n",
    "        df.show()\n",
    "        df.coalesce(1).write.csv('../data/courses_csv_pipe/', header=True, mode='overwrite', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a17baf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sep '|'\n",
    "courses_df.coalesce(1).write.csv('../data/courses_csv/', sep='|', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6ea3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|course_id|course_title               |course_published_dt|is_active|last_updated_ts              |\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "|1        |Mastering Python           |2021-01-14         |true     |2021-02-18T16:57:25.000+05:30|\n",
      "|2        |Data Engineering Essentials|2021-02-10         |true     |2021-03-05T12:07:33.000+05:30|\n",
      "|3        |Mastering PySpark          |2021-01-07         |true     |2021-04-06T10:05:42.000+05:30|\n",
      "|4        |AWS Essentials             |2021-03-19         |false    |2021-04-10T02:25:36.000+05:30|\n",
      "|5        |Docker 101                 |2021-02-28         |true     |2021-03-21T07:18:52.000+05:30|\n",
      "+---------+---------------------------+-------------------+---------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/courses_csv/', sep='|', header=True).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3969309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|course_id|course_title|course_published_dt|is_active|last_updated_ts       |\n",
      "+---------------------------------------------------------------------------+\n",
      "|1|Mastering Python|2021-01-14|true|2021-02-18T16:57:25.000+05:30           |\n",
      "|2|Data Engineering Essentials|2021-02-10|true|2021-03-05T12:07:33.000+05:30|\n",
      "|3|Mastering PySpark|2021-01-07|true|2021-04-06T10:05:42.000+05:30          |\n",
      "|4|AWS Essentials|2021-03-19|false|2021-04-10T02:25:36.000+05:30            |\n",
      "|5|Docker 101|2021-02-28|true|2021-03-21T07:18:52.000+05:30                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/courses_csv/', header=True).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e614da90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_id|course_title|course_published_dt|is_active|last_updated_ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It should include seperator ('|'), default (',')\n",
    "spark.read.csv('../data/courses_csv/', header=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7f02a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_id: string (nullable = true)\n",
      " |-- course_title: string (nullable = true)\n",
      " |-- course_published_dt: string (nullable = true)\n",
      " |-- is_active: string (nullable = true)\n",
      " |-- last_updated_ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('../data/courses_csv/', header=True, sep='|').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ea50d",
   "metadata": {},
   "source": [
    "#### option and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76506f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"order_id INT, order_date TIMESTAMP, order_customer_id INT, order_status STRING\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73c07069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'int'),\n",
       " ('order_date', 'timestamp'),\n",
       " ('order_customer_id', 'int'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders = spark.read.csv('../data/orders.csv', schema=schema)\n",
    "\n",
    "orders.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4036e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method option in module pyspark.sql.readwriter:\n",
      "\n",
      "option(key, value) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Adds an output option for the underlying data source.\n",
      "    \n",
      "    You can set the following option(s) for writing files:\n",
      "        * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      "            timestamps in the JSON/CSV datasources or partition values. The following\n",
      "            formats of `timeZone` are supported:\n",
      "    \n",
      "            * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      "            * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      "    \n",
      "            Other short names like 'CST' are not recommended to use because they can be\n",
      "            ambiguous. If it isn't set, the current value of the SQL config\n",
      "            ``spark.sql.session.timeZone`` is used by default.\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orders.write.option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c0a7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method options in module pyspark.sql.readwriter:\n",
      "\n",
      "options(**options) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Adds output options for the underlying data source.\n",
      "    \n",
      "    You can set the following option(s) for writing files:\n",
      "        * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      "            timestamps in the JSON/CSV datasources or partition values. The following\n",
      "            formats of `timeZone` are supported:\n",
      "    \n",
      "            * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      "            * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      "    \n",
      "            Other short names like 'CST' are not recommended to use because they can be\n",
      "            ambiguous. If it isn't set, the current value of the SQL config\n",
      "            ``spark.sql.session.timeZone`` is used by default.\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orders.write.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "842b797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(orders.\n",
    "coalesce(1).\n",
    "write.\n",
    "option('sep', '|').\n",
    "option('header', True).\n",
    "option('compression', 'gzip').\n",
    "csv('../data/orders_csv_pipe', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a5c8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_id', 'string'),\n",
       " ('order_date', 'string'),\n",
       " ('order_customer_id', 'string'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('../data/orders_csv_pipe/', header=True, sep='|').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2467d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(orders.\n",
    "coalesce(1).\n",
    "write.\n",
    "mode('overwrite').\n",
    "options(sep='|', header=True, compression='gzip').\n",
    "csv('../data/orders_csv_pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80ac5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# options with dictionary type\n",
    "options = {'sep': '|', 'compression':'gzip', 'header':True}\n",
    "\n",
    "(orders.\n",
    "coalesce(1).\n",
    "write.\n",
    "mode('overwrite').\n",
    "options(**options).\n",
    "csv('../data/orders_csv_pipe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d36dd",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec44742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "json('../data/courses_json', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98120677",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "format('json')\n",
    ".save('../data/courses_json', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6df15805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "|        3|         2021-01-07|   Mastering PySpark|     true|2021-04-06T10:05:...|\n",
      "|        4|         2021-03-19|      AWS Essentials|    false|2021-04-10T02:25:...|\n",
      "|        5|         2021-02-28|          Docker 101|     true|2021-03-21T07:18:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../data/courses_json/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb4f7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With compression\n",
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "json('../data/courses_json', mode='overwrite', compression='gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6b85e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|course_id|course_published_dt|        course_title|is_active|     last_updated_ts|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "|        1|         2021-01-14|    Mastering Python|     true|2021-02-18T16:57:...|\n",
      "|        2|         2021-02-10|Data Engineering ...|     true|2021-03-05T12:07:...|\n",
      "+---------+-------------------+--------------------+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('../data/courses_json/').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206f825",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cb8a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param partitionBy: names of partitioning columns\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      "                        lzo, brotli, lz4, and zstd). This will override\n",
      "                        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      "                        value specified in ``spark.sql.parquet.compression.codec``.\n",
      "    \n",
      "    >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d400db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "parquet('../data/courses_parquet', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34418f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|course_id|        course_title|course_published_dt|is_active|    last_updated_ts|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "|        1|    Mastering Python|         2021-01-14|     true|2021-02-18 16:57:25|\n",
      "|        2|Data Engineering ...|         2021-02-10|     true|2021-03-05 12:07:33|\n",
      "+---------+--------------------+-------------------+---------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('../data/courses_parquet').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1026be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('course_id', 'bigint'),\n",
       " ('course_title', 'string'),\n",
       " ('course_published_dt', 'date'),\n",
       " ('is_active', 'boolean'),\n",
       " ('last_updated_ts', 'timestamp')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/courses_parquet').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7b4ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "format('parquet').\n",
    "save('../data/courses_parquet', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b663cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param partitionBy: names of partitioning columns\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      "                        lzo, brotli, lz4, and zstd). This will override\n",
      "                        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      "                        value specified in ``spark.sql.parquet.compression.codec``.\n",
      "    \n",
      "    >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4cbbd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snappy'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3eccd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "parquet('../data/courses_parquet_no_comp', mode='overwrite', compression='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82bb9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "parquet('../data/courses_parquet_gzip', mode='overwrite', compression='gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3741ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.parquet.compression.codec', 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f04edda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'none'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.parquet.compression.codec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "595946d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(courses_df.\n",
    "coalesce(1).\n",
    "write.\n",
    "parquet('../data/courses_parquet_no_comp', mode='overwrite'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9f4ff",
   "metadata": {},
   "source": [
    "#### Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a38aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method mode in module pyspark.sql.readwriter:\n",
      "\n",
      "mode(saveMode) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Specifies the behavior when data or table already exists.\n",
      "    \n",
      "    Options include:\n",
      "    \n",
      "    * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      "    * `overwrite`: Overwrite existing data.\n",
      "    * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      "    * `ignore`: Silently ignore this operation if data already exists.\n",
      "    \n",
      "    >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(courses_df.write.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638f852",
   "metadata": {},
   "source": [
    "Different ways mode can be specified while writing dataframe into files. `file_format` can ne any valid out of the box format such as `text`, `csv`, `json`, `parquet`, `orc`.\n",
    "* `courses_df.write.mode(savemode).file_format(path_to_folder)`\n",
    "* `courses_df.write.file_format(path_to_folder, mode=savemode)`\n",
    "* `courses_df.write.mode(savemode).format(file_format).save(path_to_folder)`\n",
    "* `courses_df.write.format(file_format).save(path_to_folder, mode=savemode)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e8d3",
   "metadata": {},
   "source": [
    " * Understand default behaviour\n",
    "    * Fails if folder exists.\n",
    "    * Creates folder and then adds files to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49c50515",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/E:/Practice/PySpark/data/courses_parquet already exists.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6996/2228500036.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcourses_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/courses_parquet/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/E:/Practice/PySpark/data/courses_parquet already exists.;"
     ]
    }
   ],
   "source": [
    "# Throws an error if folder already exists\n",
    "courses_df.write.parquet('../data/courses_parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7e36f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.parquet.compression.codec', 'snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13d48b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/courses_parquet/').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1fe3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df.coalesce(1).write.mode('append').parquet('../data/courses_parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afa01107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/courses_parquet/').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
