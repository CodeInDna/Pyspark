{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062d3d1f",
   "metadata": {},
   "source": [
    "## Partitioning Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9f6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abd1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "userName = 'CodeInDNA'\n",
    "spark = SparkSession. \\\n",
    "        builder. \\\n",
    "        appName(f'{userName} - JoinSparkDF'). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7867c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.read.json('../data/orders.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe3f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e61d3067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method partitionBy in module pyspark.sql.readwriter:\n",
      "\n",
      "partitionBy(*cols) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Partitions the output by the given columns on the file system.\n",
      "    \n",
      "    If specified, the output is laid out on the file system similar\n",
      "    to Hive's partitioning scheme.\n",
      "    \n",
      "    :param cols: name of columns\n",
      "    \n",
      "    >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ordersDF.write.partitionBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fad33ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n",
      "\n",
      "json(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in JSON format\n",
      "    (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      "    specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      "                        snappy and deflate).\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at `datetime pattern`_.\n",
      "                       This applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format.\n",
      "                            Custom date formats follow the formats at `datetime pattern`_.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      "    :param encoding: specifies encoding (charset) of saved json files. If None is set,\n",
      "                    the default UTF-8 charset will be used.\n",
      "    :param lineSep: defines the line separator that should be used for writing. If None is\n",
      "                    set, it uses the default value, ``\\n``.\n",
      "    :param ignoreNullFields: Whether to ignore null fields when generating JSON objects.\n",
      "                    If None is set, it uses the default value, ``true``.\n",
      "    \n",
      "    >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json does not have the keyword argument related to partitioning\n",
    "help(ordersDF.write.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "864c502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param partitionBy: names of partitioning columns\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      "                        lzo, brotli, lz4, and zstd). This will override\n",
      "                        ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      "                        value specified in ``spark.sql.parquet.compression.codec``.\n",
      "    \n",
      "    >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parquet has the keyword argument called paritionBy \n",
    "help(ordersDF.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e6965de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method orc in module pyspark.sql.readwriter:\n",
      "\n",
      "orc(path, mode=None, partitionBy=None, compression=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param partitionBy: names of partitioning columns\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, snappy, zlib, and lzo).\n",
      "                        This will override ``orc.compress`` and\n",
      "                        ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n",
      "                        specified in ``spark.sql.orc.compression.codec``.\n",
      "    \n",
      "    >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      "    >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ordersDF.write.orc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec281293",
   "metadata": {},
   "source": [
    "#### Partition date by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fdf4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "739d494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c24719c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+---------------+\n",
      "|order_customer_id|order_date|order_id|   order_status|\n",
      "+-----------------+----------+--------+---------------+\n",
      "|            11599|  20130725|       1|         CLOSED|\n",
      "|              256|  20130725|       2|PENDING_PAYMENT|\n",
      "|            12111|  20130725|       3|       COMPLETE|\n",
      "|             8827|  20130725|       4|         CLOSED|\n",
      "|            11318|  20130725|       5|       COMPLETE|\n",
      "+-----------------+----------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ordersDF.\n",
    "withColumn('order_date', date_format('order_date', 'yyyyMMdd')).\n",
    "show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a71640ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordersDF.\n",
    "withColumn('order_date', date_format('order_date', 'yyyyMMdd')).\n",
    "coalesce(1).\n",
    "write.\n",
    "partitionBy('order_date').\n",
    "parquet('../data/orders_partitioned_by_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5892986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7eb90703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/orders_partitioned_by_date/').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f523b2d",
   "metadata": {},
   "source": [
    "#### Partition data by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75306169",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordersDF.\n",
    "withColumn('order_month', date_format('order_date', 'yyyyMM')).\n",
    "coalesce(1).\n",
    "write.\n",
    "partitionBy('order_month').\n",
    "parquet('../data/orders_partitioned_by_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "036d7ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/orders_partitioned_by_month/').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295fcc8",
   "metadata": {},
   "source": [
    "#### Partition by year, month and then day of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d07a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+----+-----+------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|year|month|day_of_month|\n",
      "+-----------------+--------------------+--------+---------------+----+-----+------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|2013|   07|          25|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|2013|   07|          25|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|2013|   07|          25|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|2013|   07|          25|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|2013|   07|          25|\n",
      "+-----------------+--------------------+--------+---------------+----+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ordersDF.\n",
    "withColumn('year', date_format('order_date', 'yyyy')).\n",
    "withColumn('month', date_format('order_date', 'MM')).\n",
    "withColumn('day_of_month', date_format('order_date', 'dd')).\n",
    "show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a7aaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordersDF.\n",
    "withColumn('year', date_format('order_date', 'yyyy')).\n",
    "withColumn('month', date_format('order_date', 'MM')).\n",
    "withColumn('day_of_month', date_format('order_date', 'dd')).\n",
    "coalesce(1).\n",
    "write.\n",
    "partitionBy('year', 'month', 'day_of_month').\n",
    "parquet('../data/orders_paritioned_by_ymd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00f8b39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../data/orders_paritioned_by_ymd/').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea185706",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"order_id INT, order_customer_id INT, order_status STRING, order_date STRING\"\"\"\n",
    "df = spark.read.parquet('../data/orders_partitioned_by_date/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92f6b776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string'),\n",
       " ('order_date', 'int')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50f4b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------------+----------+\n",
      "|order_customer_id|order_id|   order_status|order_date|\n",
      "+-----------------+--------+---------------+----------+\n",
      "|             6471|   15793|       COMPLETE|  20131103|\n",
      "|             5323|   15794|     PROCESSING|  20131103|\n",
      "|            10096|   15795|         CLOSED|  20131103|\n",
      "|            11665|   15796|       COMPLETE|  20131103|\n",
      "|             6249|   15797|PENDING_PAYMENT|  20131103|\n",
      "+-----------------+--------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedacafe",
   "metadata": {},
   "source": [
    "#### Partition Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41a4088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = (df.\n",
    "where(\"order_date=='20130725'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "734b4a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* ColumnarToRow (2)\n",
      "+- Scan parquet  (1)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [order_customer_id#931L, order_id#932L, order_status#933, order_date#934]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/E:/Practice/PySpark/data/orders_partitioned_by_date]\n",
      "PartitionFilters: [isnotnull(order_date#934), (order_date#934 = 20130725)]\n",
      "ReadSchema: struct<order_customer_id:bigint,order_id:bigint,order_status:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [4]: [order_customer_id#931L, order_id#932L, order_status#933, order_date#934]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In traditional method, first spark scan all the partitions and then apply the filter which leads to increase delay(non-optimisitic approach)\n",
    "# From Spark 3.0, Spark tries to apply filter first at the scanning stage which leads to \"no more unnecessary scans\" \n",
    "sum_df.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2581fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+---------------+----------+\n",
      "|order_customer_id|order_id|   order_status|order_date|\n",
      "+-----------------+--------+---------------+----------+\n",
      "|            11599|       1|         CLOSED|  20130725|\n",
      "|              256|       2|PENDING_PAYMENT|  20130725|\n",
      "|            12111|       3|       COMPLETE|  20130725|\n",
      "|             8827|       4|         CLOSED|  20130725|\n",
      "|            11318|       5|       COMPLETE|  20130725|\n",
      "+-----------------+--------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
