{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35800014",
   "metadata": {},
   "source": [
    "## Spark SQL Functions\n",
    "\n",
    "* Spark provides robust set of pre-defined functions as part of `pyspark.sql.functions`.\n",
    "* However, they miight not fulfill all our requirements.\n",
    "* At times, we might have to develop custom UDFs for these scenarios.\n",
    "    * No function available for our requirement while applying row level transformations.\n",
    "    * Also, we might have to use multiple functions sue to which readability of the code is compromised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0892e",
   "metadata": {},
   "source": [
    "Here are the steps we need to follow to develop and use Spark User Defined Functions.\n",
    "* Develop required logic using Python as programming language.\n",
    "* Register the function using `spark.udf.register`. also assign it to a variable.\n",
    "* Variable can be used as a part of Dataframe APIs such as `select`, `filter`, etc.\n",
    "* When we register, we register with a name. That name can ne used as part of `selectExpr`or as part of Spark SQL queries using `spark.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29396054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = 'CodeInDNA'\n",
    "spark = SparkSession.builder.appName(f'SparkUDFs - {user_name}').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d1e87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method register in module pyspark.sql.udf:\n",
      "\n",
      "register(name, f, returnType=None) method of pyspark.sql.udf.UDFRegistration instance\n",
      "    Register a Python function (including lambda function) or a user-defined function\n",
      "    as a SQL function.\n",
      "    \n",
      "    :param name: name of the user-defined function in SQL statements.\n",
      "    :param f: a Python function, or a user-defined function. The user-defined function can\n",
      "        be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n",
      "        :meth:`pyspark.sql.functions.pandas_udf`.\n",
      "    :param returnType: the return type of the registered user-defined function. The value can\n",
      "        be either a :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    :return: a user-defined function.\n",
      "    \n",
      "    To register a nondeterministic Python function, users need to first build\n",
      "    a nondeterministic user-defined function for the Python function and then register it\n",
      "    as a SQL function.\n",
      "    \n",
      "    `returnType` can be optionally specified when `f` is a Python function but not\n",
      "    when `f` is a user-defined function. Please see below.\n",
      "    \n",
      "    1. When `f` is a Python function:\n",
      "    \n",
      "        `returnType` defaults to string type and can be optionally specified. The produced\n",
      "        object must match the specified type. In this case, this API works as if\n",
      "        `register(name, f, returnType=StringType())`.\n",
      "    \n",
      "        >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
      "        >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n",
      "        [Row(stringLengthString(test)='4')]\n",
      "    \n",
      "        >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n",
      "        [Row(stringLengthString(text)='3')]\n",
      "    \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "        >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "        [Row(stringLengthInt(test)=4)]\n",
      "    \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      "        >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      "        [Row(stringLengthInt(test)=4)]\n",
      "    \n",
      "    2. When `f` is a user-defined function:\n",
      "    \n",
      "        Spark uses the return type of the given user-defined function as the return type of\n",
      "        the registered user-defined function. `returnType` should not be specified.\n",
      "        In this case, this API works as if `register(name, f)`.\n",
      "    \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> from pyspark.sql.functions import udf\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> _ = spark.udf.register(\"slen\", slen)\n",
      "        >>> spark.sql(\"SELECT slen('test')\").collect()\n",
      "        [Row(slen(test)=4)]\n",
      "    \n",
      "        >>> import random\n",
      "        >>> from pyspark.sql.functions import udf\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n",
      "        >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n",
      "        >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n",
      "        [Row(random_udf()=82)]\n",
      "    \n",
      "        >>> import pandas as pd  # doctest: +SKIP\n",
      "        >>> from pyspark.sql.functions import pandas_udf\n",
      "        >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "        ... def add_one(s: pd.Series) -> pd.Series:\n",
      "        ...     return s + 1\n",
      "        ...\n",
      "        >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n",
      "        >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n",
      "        [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n",
      "    \n",
      "        >>> @pandas_udf(\"integer\")  # doctest: +SKIP\n",
      "        ... def sum_udf(v: pd.Series) -> int:\n",
      "        ...     return v.sum()\n",
      "        ...\n",
      "        >>> _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n",
      "        >>> q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n",
      "        >>> spark.sql(q).collect()  # doctest: +SKIP\n",
      "        [Row(sum_udf(v1)=1), Row(sum_udf(v1)=5)]\n",
      "    \n",
      "        .. note:: Registration for a user-defined function (case 2.) was added from\n",
      "            Spark 2.3.0.\n",
      "    \n",
      "    .. versionadded:: 1.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.udf.register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ff699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.read.json('../data/orders.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d818bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f2aae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('order_customer_id', 'bigint'),\n",
       " ('order_date', 'string'),\n",
       " ('order_id', 'bigint'),\n",
       " ('order_status', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1afd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = spark.udf.register('date_convert', lambda x: x[:10].replace('-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62f5a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|order_date|\n",
      "+----------+\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ordersDF.\n",
    "select(dc('order_date').alias('order_date')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15f80232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.filter(dc('order_date') == 20140101).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71fc8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|order_date|order_count|\n",
      "+----------+-----------+\n",
      "|  20140413|        117|\n",
      "|  20130919|        206|\n",
      "|  20140303|        266|\n",
      "|  20140410|        252|\n",
      "|  20140512|        246|\n",
      "|  20140530|        102|\n",
      "|  20140711|        138|\n",
      "|  20140202|        192|\n",
      "|  20140310|        235|\n",
      "|  20130809|        125|\n",
      "|  20130817|        253|\n",
      "|  20131015|        174|\n",
      "|  20140114|        209|\n",
      "|  20140505|        171|\n",
      "|  20140709|        150|\n",
      "|  20131029|        128|\n",
      "|  20140130|        254|\n",
      "|  20130824|        265|\n",
      "|  20130913|        103|\n",
      "|  20140610|        137|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "( ordersDF.\n",
    "    groupBy(dc('order_date').alias('order_date')).\n",
    "    count().\n",
    "    withColumnRenamed('count', 'order_count').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b8287c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|order_date|\n",
      "+----------+\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "|  20130725|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.selectExpr('date_convert(order_date) AS order_date').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b09547ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77a06054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|order_date|\n",
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|  20130725|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|  20130725|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|  20130725|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|  20130725|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|  20130725|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|  20130725|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|  20130725|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|  20130725|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|  20130725|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|  20130725|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|  20130725|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|  20130725|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|  20130725|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|  20130725|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|  20130725|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|  20130725|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|  20130725|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|  20130725|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|  20130725|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|  20130725|\n",
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT o.*, date_convert(order_date) AS order_date FROM orders o\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7039a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|order_date|\n",
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|  20140101|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|  20140101|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|  20140101|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|  20140101|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|  20140101|\n",
      "+-----------------+--------------------+--------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT o.*, date_convert(order_date) AS order_date FROM orders o WHERE date_convert(order_date)=20140101\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a807b860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+\n",
      "|date_convert(order_date)|order_date|\n",
      "+------------------------+----------+\n",
      "|                20140413|       117|\n",
      "|                20130919|       206|\n",
      "|                20140303|       266|\n",
      "|                20140410|       252|\n",
      "|                20140512|       246|\n",
      "+------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date_convert(order_date), count(*) AS order_date FROM orders o GROUP BY 1\"\"\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
