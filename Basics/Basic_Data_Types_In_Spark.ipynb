{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281941ad",
   "metadata": {},
   "source": [
    "## Overview of basic datatypes in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32927e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b2f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('SparkDataTypes') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62dcfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"first_name\": \"Pheobe\",\n",
    "                \"last_name\": \"Buffay\",\n",
    "                \"email\": \"pheobebuffay@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 13),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"first_name\": \"Joey\",\n",
    "                \"last_name\": \"Tribbiani\",\n",
    "                \"email\": \"joey@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 900.0,\n",
    "                \"customer_from\": datetime.date(2021, 2, 14),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"first_name\": \"Monica\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"email\": \"monica@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.90,\n",
    "                \"customer_from\": datetime.date(2021, 2, 22),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 28, 7, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"first_name\": \"Ross\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"email\": \"ross@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1200.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 19),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"first_name\": \"Rachel\",\n",
    "                \"last_name\": \"Green\",\n",
    "                \"email\": \"rachel@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": None,\n",
    "                \"customer_from\": datetime.date(2021, 2, 24),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"first_name\": \"Chandler\",\n",
    "                \"last_name\": \"Bing\",\n",
    "                \"email\": \"bing@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.80,\n",
    "                \"customer_from\": datetime.date(2021, 2, 22),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 25, 7, 33, 0)\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931695ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type automatically get derived even if we do not specify \n",
    "user_df = spark.createDataFrame([Row(**user) for user in users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c33c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a2000e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Pheobe|   Buffay|pheobebuffay@abc.com|       true|    1000.55|   2021-01-13|2021-02-10 01:15:00|\n",
      "|  2|      Joey|Tribbiani|        joey@abc.com|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Monica|   Geller|      monica@abc.com|       true|     1000.9|   2021-02-22|2021-02-28 07:33:00|\n",
      "|  4|      Ross|   Geller|        ross@abc.com|       true|    1200.55|   2021-01-19|2021-02-18 01:10:00|\n",
      "|  5|    Rachel|    Green|      rachel@abc.com|       true|       null|   2021-02-24|2021-02-18 03:33:00|\n",
      "|  6|  Chandler|     Bing|        bing@abc.com|       true|     1000.8|   2021-02-22|2021-02-25 07:33:00|\n",
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9661b63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, f_name: string, l_name: string, email: string, is_cust: boolean, amt_paid: float, cust_frm: date, last_datetime: timestamp]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe from list of dict using custom name and datatype\n",
    "spark.createDataFrame([Row(*user.values()) for user in users], 'id int, f_name string, l_name string, email string, is_cust boolean, amt_paid float, cust_frm date, last_datetime timestamp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa295e",
   "metadata": {},
   "source": [
    "### Specify schema for spark dataframe using string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40abb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "            (\n",
    "                 1,\n",
    "                 \"Pheobe\",\n",
    "                 \"Buffay\",\n",
    "                 \"pheobebuffay@abc.com\",\n",
    "                 True,\n",
    "                 1000.55,\n",
    "                 datetime.date(2021, 1, 13),\n",
    "                 datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "            ),\n",
    "            (\n",
    "                 2,\n",
    "                 \"Joey\",\n",
    "                 \"Tribbiani\",\n",
    "                 \"joey@abc.com\",\n",
    "                 True,\n",
    "                 900.0,\n",
    "                 datetime.date(2021, 2, 14),\n",
    "                 datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            ),\n",
    "            (\n",
    "                 3,\n",
    "                 \"Monica\",\n",
    "                 \"Geller\",\n",
    "                 \"monica@abc.com\",\n",
    "                 True,\n",
    "                 1000.90,\n",
    "                 datetime.date(2021, 2, 22),\n",
    "                 datetime.datetime(2021, 2, 28, 7, 33, 0)\n",
    "            ),\n",
    "            (\n",
    "                 4,\n",
    "                 \"Ross\",\n",
    "                 \"Geller\",\n",
    "                 \"ross@abc.com\",\n",
    "                 True,\n",
    "                 1200.55,\n",
    "                 datetime.date(2021, 1, 19),\n",
    "                 datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            ),\n",
    "            (\n",
    "                 5,\n",
    "                 \"Rachel\",\n",
    "                 \"Green\",\n",
    "                 \"rachel@abc.com\",\n",
    "                 True,\n",
    "                 None,\n",
    "                 datetime.date(2021, 2, 24),\n",
    "                 datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            ),\n",
    "            (\n",
    "                 6,\n",
    "                 \"Chandler\",\n",
    "                 \"Bing\",\n",
    "                 \"bing@abc.com\",\n",
    "                 True,\n",
    "                 1000.80,\n",
    "                 datetime.date(2021, 2, 22),\n",
    "                 datetime.datetime(2021, 2, 25, 7, 33, 0)\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0926409",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = \"\"\"\n",
    "              id INT,\n",
    "              first_name STRING,\n",
    "              last_name STRING,\n",
    "              email STRING,\n",
    "              is_customer BOOLEAN,\n",
    "              amount_paid FLOAT,\n",
    "              customer_from DATE,\n",
    "              last_updated_ts TIMESTAMP\n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dba1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "| id|first_name|last_name|               email|is_customer|amount_paid|customer_from|    last_updated_ts|\n",
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "|  1|    Pheobe|   Buffay|pheobebuffay@abc.com|       true|    1000.55|   2021-01-13|2021-02-10 01:15:00|\n",
      "|  2|      Joey|Tribbiani|        joey@abc.com|       true|      900.0|   2021-02-14|2021-02-18 03:33:00|\n",
      "|  3|    Monica|   Geller|      monica@abc.com|       true|     1000.9|   2021-02-22|2021-02-28 07:33:00|\n",
      "|  4|      Ross|   Geller|        ross@abc.com|       true|    1200.55|   2021-01-19|2021-02-18 01:10:00|\n",
      "|  5|    Rachel|    Green|      rachel@abc.com|       true|       null|   2021-02-24|2021-02-18 03:33:00|\n",
      "|  6|  Chandler|     Bing|        bing@abc.com|       true|     1000.8|   2021-02-22|2021-02-25 07:33:00|\n",
      "+---+----------+---------+--------------------+-----------+-----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=user_schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa88a869",
   "metadata": {},
   "source": [
    "### Specify schema for spark dataframe using list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d092167",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = [\n",
    "              'id INT',\n",
    "              'first_name STRING',\n",
    "              'last_name STRING',\n",
    "              'email STRING',\n",
    "              'is_customer BOOLEAN',\n",
    "              'amount_paid FLOAT',\n",
    "              'customer_from DATE',\n",
    "              'last_updated_ts TIMESTAMP'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7bca5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
      "|id INT|first_name STRING|last_name STRING|        email STRING|is_customer BOOLEAN|amount_paid FLOAT|customer_from DATE|last_updated_ts TIMESTAMP|\n",
      "+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
      "|     1|           Pheobe|          Buffay|pheobebuffay@abc.com|               true|          1000.55|        2021-01-13|      2021-02-10 01:15:00|\n",
      "|     2|             Joey|       Tribbiani|        joey@abc.com|               true|            900.0|        2021-02-14|      2021-02-18 03:33:00|\n",
      "|     3|           Monica|          Geller|      monica@abc.com|               true|           1000.9|        2021-02-22|      2021-02-28 07:33:00|\n",
      "|     4|             Ross|          Geller|        ross@abc.com|               true|          1200.55|        2021-01-19|      2021-02-18 01:10:00|\n",
      "|     5|           Rachel|           Green|      rachel@abc.com|               true|             null|        2021-02-24|      2021-02-18 03:33:00|\n",
      "|     6|         Chandler|            Bing|        bing@abc.com|               true|           1000.8|        2021-02-22|      2021-02-25 07:33:00|\n",
      "+------+-----------------+----------------+--------------------+-------------------+-----------------+------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=user_schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d17544",
   "metadata": {},
   "source": [
    "### Specify schema for spark dataframe using spark types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9d1031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = StructType([\n",
    "                  StructField('id', IntegerType()),\n",
    "                  StructField('first_name', StringType()),\n",
    "                  StructField('last_name', StringType()),\n",
    "                  StructField('email', StringType()),\n",
    "                  StructField('is_customer', BooleanType()),\n",
    "                  StructField('amount_paid', FloatType()),\n",
    "                  StructField('customer_from', DateType()),\n",
    "                  StructField('last_updated_ts', TimestampType()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01e6996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, first_name: string, last_name: string, email: string, is_customer: boolean, amount_paid: float, customer_from: date, last_updated_ts: timestamp]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(users, schema=user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e0c002c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f21721e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    :param data: an RDD of any kind of SQL data representation (e.g. row, tuple, int, boolean,\n",
      "        etc.), :class:`list`, or :class:`pandas.DataFrame`.\n",
      "    :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is ``None``.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
      "        ``int`` as a short name for ``IntegerType``.\n",
      "    :param samplingRatio: the sample ratio of rows used for inferring\n",
      "    :param verifySchema: verify data types of every row against schema.\n",
      "    :return: :class:`DataFrame`\n",
      "    \n",
      "    .. versionchanged:: 2.1\n",
      "       Added verifySchema.\n",
      "    \n",
      "    .. note:: Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    .. note:: When Arrow optimization is enabled, strings inside Pandas DataFrame in Python\n",
      "        2 are converted into bytes as they are bytes in Python 2 whereas regular strings are\n",
      "        left as strings. When using strings in Python 2, use unicode `u\"\"` as Python standard\n",
      "        practice.\n",
      "    \n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
