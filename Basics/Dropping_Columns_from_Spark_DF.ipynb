{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c6ce6e",
   "metadata": {},
   "source": [
    "## Dropping columns from spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204a5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DropColumnsSpark').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6335c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"first_name\": \"Pheobe\",\n",
    "                \"last_name\": \"Buffay\",\n",
    "                \"phone_numbers\": Row(mobile= \"82349238942\", home= \"2348910249\", office= \"8273929\", shop=None),\n",
    "                \"courses\": [1, 3, 5, 7],\n",
    "                \"email\": \"pheobebuffay@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 13),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"first_name\": \"Joey\",\n",
    "                \"last_name\": \"Tribbiani\",\n",
    "                \"phone_numbers\": Row(mobile= \"82349238942\", home= \"2348910249\", office= None, shop=None),\n",
    "                \"courses\": [2, 4, 5],\n",
    "                \"email\": \"joey@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 900.0,\n",
    "                \"customer_from\": datetime.date(2021, 2, 14),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"first_name\": \"Monica\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"phone_numbers\": Row(mobile= None, home= None, office= None, shop=None),\n",
    "                \"courses\": [2],\n",
    "                \"email\": \"monica@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.90,\n",
    "                \"customer_from\": datetime.date(2021, 2, 22),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 28, 7, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"first_name\": \"Ross\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"phone_numbers\": Row(mobile= \"82349238942\", home= None, office= None, shop=None),\n",
    "                \"courses\": [],\n",
    "                \"email\": \"ross@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1200.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 19),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"first_name\": \"Rachel\",\n",
    "                \"last_name\": \"Green\",\n",
    "                \"phone_numbers\": Row(mobile= \"82349238942\", home= \"2348910249\", office= \"8273929\", shop= \"5343434654\"),\n",
    "                \"courses\": [3],\n",
    "                \"email\": \"rachel@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": None,\n",
    "                \"customer_from\": datetime.date(2021, 2, 24),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"first_name\": \"Chandler\",\n",
    "                \"last_name\": \"Bing\",\n",
    "                \"phone_numbers\": Row(mobile= \"8273929\", home= None, office= None, shop=None),\n",
    "                \"courses\": [2, 4],\n",
    "                \"email\": \"bing@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.80,\n",
    "                \"customer_from\": datetime.date(2021, 2, 22),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 25, 7, 33, 0)\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15943fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF = spark.createDataFrame(users)\n",
    "\n",
    "usersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5670da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop in module pyspark.sql.dataframe:\n",
      "\n",
      "drop(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` that drops the specified column.\n",
      "    This is a no-op if schema doesn't contain the given column name(s).\n",
      "    \n",
      "    :param cols: a string name of the column to drop, or a\n",
      "        :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      "    \n",
      "    >>> df.drop('age').collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.drop(df.age).collect()\n",
      "    [Row(name='Alice'), Row(name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      "    [Row(age=5, height=85, name='Bob')]\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      "    [Row(age=5, name='Bob', height=85)]\n",
      "    \n",
      "    >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      "    [Row(name='Bob')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(usersDF.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6dec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.drop('last_updated_ts').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa898a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.drop(usersDF['last_updated_ts']).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d33c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.drop(col('last_updated_ts')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08da2447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we have column name which does not exist, the column will be ignored\n",
    "usersDF.drop(col('users_id')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c024595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Multiple columns\n",
    "usersDF.drop('first_name', 'last_name').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e2bb35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "each col in the param list should be a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14676/937527981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This will fail as we are passing multiple column objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# When we want to pass more than one column, we have to pass all column names as strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0musersDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'first_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'last_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2145\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2146\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2147\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"each col in the param list should be a string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2148\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: each col in the param list should be a string"
     ]
    }
   ],
   "source": [
    "# This will fail as we are passing multiple column objects\n",
    "# When we want to pass more than one column, we have to pass all column names as strings\n",
    "usersDF.drop(col('first_name'), col('last_name')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1cf9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.drop(col('first_name')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9150521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount_paid: double (nullable = true)\n",
      " |-- courses: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- customer_from: date (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- is_customer: boolean (nullable = true)\n",
      " |-- last_updated_ts: timestamp (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- home: string (nullable = true)\n",
      " |    |-- office: string (nullable = true)\n",
      " |    |-- shop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If we have column name which does not exist, the column will be ignored\n",
    "usersDF.drop('user_id', 'first_name', 'last_name').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d695bbc",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "* `drop` accepts column names separated by string.\n",
    "* If we are passing more than one column object, it will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094eff9c",
   "metadata": {},
   "source": [
    "#### Drop duplicate records\n",
    "\n",
    "* Drop duplicates based on all columns, it is known as distinct.\n",
    "* Drop duplicates based on certain columns.\n",
    "* We can use `distinct`, `drop_duplicates` or `dropDuplicates` for both scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4661409",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"first_name\": \"Pheobe\",\n",
    "                \"last_name\": \"Buffay\",\n",
    "                \"email\": \"pheobebuffay@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 13),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"first_name\": \"Joey\",\n",
    "                \"last_name\": \"Tribbiani\",\n",
    "                \"email\": \"joey@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 900.0,\n",
    "                \"customer_from\": datetime.date(2021, 2, 14),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"first_name\": \"Joey\",\n",
    "                \"last_name\": \"Tribbiani\",\n",
    "                \"email\": \"joey@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.0,\n",
    "                \"customer_from\": datetime.date(2021, 2, 14),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"first_name\": \"Ross\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"email\": \"ross@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1200.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 19),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"first_name\": \"Ross\",\n",
    "                \"last_name\": \"Geller\",\n",
    "                \"email\": \"ross@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1200.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 19),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c0d0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF = spark.createDataFrame(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b46f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name| id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|  1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|     1000.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|    1200.55|   2021-01-19|        ross@abc.com|      Ross|  4|       true|   Geller|2021-02-18 01:10:00|\n",
      "|    1200.55|   2021-01-19|        ross@abc.com|      Ross|  4|       true|   Geller|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81032cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b82272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method distinct in module pyspark.sql.dataframe:\n",
      "\n",
      "distinct() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      "    \n",
      "    >>> df.distinct().count()\n",
      "    2\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(usersDF.distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8387a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name| id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|    1200.55|   2021-01-19|        ross@abc.com|      Ross|  4|       true|   Geller|2021-02-18 01:10:00|\n",
      "|     1000.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|  1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop exact duplicates\n",
    "usersDF.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78a0bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(usersDF.drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c55d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method dropDuplicates in module pyspark.sql.dataframe:\n",
      "\n",
      "dropDuplicates(subset=None) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Return a new :class:`DataFrame` with duplicate rows removed,\n",
      "    optionally only considering certain columns.\n",
      "    \n",
      "    For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      "    :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      "    duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      "    be and system will accordingly limit the state. In addition, too late data older than\n",
      "    watermark will be dropped to avoid any possibility of duplicates.\n",
      "    \n",
      "    :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = sc.parallelize([ \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=5, height=80), \\\n",
      "    ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      "    >>> df.dropDuplicates().show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    |  5|    80|Alice|\n",
      "    | 10|    80|Alice|\n",
      "    +---+------+-----+\n",
      "    \n",
      "    >>> df.dropDuplicates(['name', 'height']).show()\n",
      "    +---+------+-----+\n",
      "    |age|height| name|\n",
      "    +---+------+-----+\n",
      "    |  5|    80|Alice|\n",
      "    +---+------+-----+\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(usersDF.dropDuplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7eb4cb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonUtils.toSeq. Trace:\npy4j.Py4JException: Method toSeq([class java.lang.String]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14676/18544001.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We can also drop duplicates based on certain columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This will fail as the function expects sequence type object such as list or array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0musersDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mdropDuplicates\u001b[1;34m(self, subset)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[1;34m(self, cols, converter)\u001b[0m\n\u001b[0;32m   1200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 raise Py4JError(\n\u001b[0m\u001b[0;32m    331\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonUtils.toSeq. Trace:\npy4j.Py4JException: Method toSeq([class java.lang.String]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:339)\r\n\tat py4j.Gateway.invoke(Gateway.java:276)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\n"
     ]
    }
   ],
   "source": [
    "# We can also drop duplicates based on certain columns\n",
    "# This will fail as the function expects sequence type object such as list or array\n",
    "usersDF.dropDuplicates('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF.dropDuplicates(['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF.dropDuplicates(['id', 'amount_paid']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cca4d",
   "metadata": {},
   "source": [
    "#### Dropping Null bases records from spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd1714",
   "metadata": {},
   "source": [
    "* Drop records where all column value are nulls.\n",
    "* Drop records any of the column value is null.\n",
    "* Drop records that have less than `thresh` non-null values.\n",
    "* Drop records when any of the column value or all column values are nulls for provided subset of columns.\n",
    "* We can use `df.na.drop` or `df.dropna` to take care of dealing with records having columns with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"first_name\": \"Pheobe\",\n",
    "                \"last_name\": \"Buffay\",\n",
    "                \"email\": \"pheobebuffay@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 1000.55,\n",
    "                \"customer_from\": datetime.date(2021, 1, 13),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"first_name\": \"Joey\",\n",
    "                \"last_name\": \"Tribbiani\",\n",
    "                \"email\": \"joey@abc.com\",\n",
    "                \"is_customer\": True,\n",
    "                \"amount_paid\": 900.0,\n",
    "                \"customer_from\": datetime.date(2021, 2, 14),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": None,\n",
    "                \"first_name\": None,\n",
    "                \"last_name\": None,\n",
    "                \"email\": None,\n",
    "                \"is_customer\": None,\n",
    "                \"amount_paid\": None,\n",
    "                \"customer_from\": None,\n",
    "                \"last_updated_ts\": None\n",
    "            },\n",
    "            {\n",
    "                \"id\": None,\n",
    "                \"first_name\": None,\n",
    "                \"last_name\": None,\n",
    "                \"email\": \"ross@abc.com\",\n",
    "                \"is_customer\": None,\n",
    "                \"amount_paid\": None,\n",
    "                \"customer_from\": datetime.date(2021, 1, 19),\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"first_name\": None,\n",
    "                \"last_name\": None,\n",
    "                \"email\": None,\n",
    "                \"is_customer\": None,\n",
    "                \"amount_paid\": None,\n",
    "                \"customer_from\": None,\n",
    "                \"last_updated_ts\": datetime.datetime(2021, 2, 18, 1, 10, 0)\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fef6d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name|  id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|   1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|   2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|       null|         null|                null|      null|null|       null|     null|               null|\n",
      "|       null|   2021-01-19|        ross@abc.com|      null|null|       null|     null|2021-02-18 01:10:00|\n",
      "|       null|         null|                null|      null|   4|       null|     null|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF = spark.createDataFrame(users)\n",
    "usersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0c13621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameNaFunctions in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrameNaFunctions(builtins.object)\n",
      " |  DataFrameNaFunctions(df)\n",
      " |  \n",
      " |  Functionality for working with missing data in :class:`DataFrame`.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  drop(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  fill(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(usersDF.na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2086359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name| id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|  1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "533f71c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name|  id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|   1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|   2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|       null|   2021-01-19|        ross@abc.com|      null|null|       null|     null|2021-02-18 01:10:00|\n",
      "|       null|         null|                null|      null|   4|       null|     null|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5160fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name|  id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|   1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|   2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|       null|   2021-01-19|        ross@abc.com|      null|null|       null|     null|2021-02-18 01:10:00|\n",
      "|       null|         null|                null|      null|   4|       null|     null|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "157e0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name|  id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|   1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|   2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|       null|   2021-01-19|        ross@abc.com|      null|null|       null|     null|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abf395e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name|  id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|   1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|   2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "|       null|   2021-01-19|        ross@abc.com|      null|null|       null|     null|2021-02-18 01:10:00|\n",
      "|       null|         null|                null|      null|   4|       null|     null|2021-02-18 01:10:00|\n",
      "+-----------+-------------+--------------------+----------+----+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop(how='all', subset=['id', 'email']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "355fb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|amount_paid|customer_from|               email|first_name| id|is_customer|last_name|    last_updated_ts|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "|    1000.55|   2021-01-13|pheobebuffay@abc.com|    Pheobe|  1|       true|   Buffay|2021-02-10 01:15:00|\n",
      "|      900.0|   2021-02-14|        joey@abc.com|      Joey|  2|       true|Tribbiani|2021-02-18 03:33:00|\n",
      "+-----------+-------------+--------------------+----------+---+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.na.drop(how='any', subset=['id', 'email']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
