{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb06aa54",
   "metadata": {},
   "source": [
    "# Processing Column Data\n",
    "\n",
    "We will explore the functions available under pyspark.sql.functions to derive new values from existing column values within a dataframe.\n",
    "* [Pre-defined Functions](#first)\n",
    "* [Create dummy dataframe](#sec)\n",
    "* [Categories of functions](#third)\n",
    "* [Special fucntions - col and lit](#fourth)\n",
    "* [String manipulation functions - 1](#fifth)\n",
    "* [String manipulation functions - 2](#sixth)\n",
    "* [Date and Time overview](#seventh)\n",
    "* [Date and Time arithmetic](#eigth)\n",
    "* [Date and Time - trunc and date_trunc](#ninth)\n",
    "* [Date and Time - extracting information](#tenth)\n",
    "* [Dealing with UNIX timestamp](#eleventh)\n",
    "* [Example - word count](#twelth)\n",
    "* [Conclusion](#thirdteenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc39dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78998558",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkFunctions').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ebf74",
   "metadata": {},
   "source": [
    "### Pre-defined Functions<a id=\"first\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace95291",
   "metadata": {},
   "source": [
    "We typically process data in the columns using functions in `pyspark.sl.functions`. Let us understand details about these functions in detail as part of this module.\n",
    "\n",
    "* Let us recap about functions or API to proces DataFrames.\n",
    "    * Projection - `select` or `withColumn` or `drop` or `selectExpr`\n",
    "    * Filtering - `filter` or `where`\n",
    "    * Grouping data by key and perform aggregations - `groupBy`\n",
    "    * Sorting data - `sort` or `orderBy`\n",
    "* We can pass column names or literals or expressions to all the dataframe APIs.\n",
    "* Expressions include arithmetic operations, transformations using functions from `pyspark.sql.functions`.\n",
    "* There are approximately 300 functions under `pyspark.sql.functions`.\n",
    "* There are some important functions related to String Manipulation, Date Manipulation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54af4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.csv('../data/orders.csv', \n",
    "        schema='order_id INT, order_date STRING, order_customer_id INT, order_status STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76f2c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59f7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eeb17",
   "metadata": {},
   "source": [
    "#### date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce64bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_format in module pyspark.sql.functions:\n",
      "\n",
      "date_format(date, format)\n",
      "    Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "    format given by the second argument.\n",
      "    \n",
      "    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "    pattern letters of `datetime pattern`_. can be used.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "        specialized implementation.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "    [Row(date='04/08/2015')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7a412b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dateformat with select and alias\n",
    "orders.select('*',\n",
    "             date_format('order_date', 'yyyyMM').alias('order_month')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60ce72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     201307|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|     201307|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|     201307|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|     201307|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|     201307|\n",
      "+--------+--------------------+-----------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dateformat with withColumn\n",
    "orders.withColumn('order_month', date_format('order_date', 'yyyyMM')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346c78f",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adcefdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class filter in module builtins:\n",
      "\n",
      "class filter(object)\n",
      " |  filter(function or None, iterable) --> filter object\n",
      " |  \n",
      " |  Return an iterator yielding those items of iterable for which function(item)\n",
      " |  is true. If function is None, return the items that are true.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __next__(self, /)\n",
      " |      Implement next(self).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee5e32",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbded0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupBy in module pyspark.sql.dataframe:\n",
      "\n",
      "groupBy(*cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Groups the :class:`DataFrame` using the specified columns,\n",
      "    so we can run aggregation on them. See :class:`GroupedData`\n",
      "    for all the available aggregate functions.\n",
      "    \n",
      "    :func:`groupby` is an alias for :func:`groupBy`.\n",
      "    \n",
      "    :param cols: list of columns to group by.\n",
      "        Each element should be a column name (string) or an expression (:class:`Column`).\n",
      "    \n",
      "    >>> df.groupBy().avg().collect()\n",
      "    [Row(avg(age)=3.5)]\n",
      "    >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(df.name).avg().collect())\n",
      "    [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      "    >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      "    [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(orders.groupBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87dbb3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|   25876|2014-01-01 00:00:...|             3414|PENDING_PAYMENT|\n",
      "|   25877|2014-01-01 00:00:...|             5549|PENDING_PAYMENT|\n",
      "|   25878|2014-01-01 00:00:...|             9084|        PENDING|\n",
      "|   25879|2014-01-01 00:00:...|             5118|        PENDING|\n",
      "|   25880|2014-01-01 00:00:...|            10146|       CANCELED|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function as a part of where or filter\n",
    "orders.filter(date_format('order_date', 'yyyyMM') == 201401).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0401c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|order_month|count|\n",
      "+-----------+-----+\n",
      "|     201401| 5908|\n",
      "|     201405| 5467|\n",
      "|     201312| 5892|\n",
      "|     201310| 5335|\n",
      "|     201311| 6381|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function as a part of groupBy\n",
    "orders.groupBy(date_format('order_date', 'yyyyMM').alias('order_month')).count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3b917",
   "metadata": {},
   "source": [
    "### Create dummy dataframe<a id=\"sec\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89db5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X', )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112b65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(l, 'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8e2d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "330c2329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3808b3",
   "metadata": {},
   "source": [
    "Once dataframe is created, we can use to understand how to use functions. for examples, to get current date, we can run `df.select(current_date()).show()`\n",
    "\n",
    "It is similar to Oracle Query `SELECT sysdate FROM dual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2939b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2022-01-27|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date().alias('current_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0eeea",
   "metadata": {},
   "source": [
    "Here is the example of creating dataframe using collection of employees. we will be using this dataframe to explore all the important functions to process column data in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d453f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", 'Tiger', 1000.0, \"USA\", \"+1 123 456 7890\", \"123 45 6789\"),\n",
    "            (2, \"Henry\", 'Ford', 750.0, \"UK\", \"+1 123 456 7890\", \"123 45 6789\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "361ff2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF = spark.createDataFrame(employees, schema=\"\"\"id INT, f_name STRING, l_name STRING, salary FLOAT, nationality STRING, ph_no STRING, ssn STRING\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95e4d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- f_name: string (nullable = true)\n",
      " |-- l_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- ph_no: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41143ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|id |f_name|l_name|salary|nationality|ph_no          |ssn        |\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|1  |Scott |Tiger |1000.0|USA        |+1 123 456 7890|123 45 6789|\n",
      "|1  |Henry |Ford  |750.0 |UK         |+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a810d7",
   "metadata": {},
   "source": [
    "### Categories of functions<a id=\"third\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f89f9",
   "metadata": {},
   "source": [
    "There are approx 300 functions under `pyspark.sql.functions`. At a higher level they canbe grouped into a few categories.\n",
    "\n",
    "* String Manipulation\n",
    "    * Case conversion - `lower`, `upper`\n",
    "    * Getting length - `length`\n",
    "    * Extracting substrings - `substring`, `split`\n",
    "    * Trimming - `trim`, `ltrim`, `rtrim`\n",
    "    * Padding - `lpad`, `rpad`\n",
    "    * Concatenating string - `concat`, `concat_ws`\n",
    "* Date Manipulation\n",
    "    * Getting current date and time - `current_date`, `current_timestamp`\n",
    "    * Date arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n",
    "    * Begining and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n",
    "    * Formatting Date - `date_format`\n",
    "    * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n",
    "* Aggregate Functions\n",
    "    * `count`, `countDistinct`\n",
    "    * `sum`, `avg`\n",
    "    * `min`, `max`\n",
    "* Other Functions - We will explore depending on the use cases.\n",
    "    * `CASE` and `WHEN`\n",
    "    * `CAST` for type casting\n",
    "    * Functions to manage special types such as `ARRAY`, `MAP`, `STRUCT` type columns\n",
    "    * Many others "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7774e",
   "metadata": {},
   "source": [
    "### Special functions - col and lit<a id=\"fourth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d584852",
   "metadata": {},
   "source": [
    "* For dataframe APIs such as `select`, `groupBy`, `orderBy` etc we can pass column names as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c236ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|f_name|l_name|\n",
      "+------+------+\n",
      "| Scott| Tiger|\n",
      "| Henry|  Ford|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(\"f_name\", \"l_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48564a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|nationality|count|\n",
      "+-----------+-----+\n",
      "|        USA|    1|\n",
      "|         UK|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.groupBy(\"nationality\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af2e1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "| id|f_name|l_name|salary|nationality|          ph_no|        ssn|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|  2| Henry|  Ford| 750.0|         UK|+1 123 456 7890|123 45 6789|\n",
      "|  1| Scott| Tiger|1000.0|        USA|+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f4c98",
   "metadata": {},
   "source": [
    "* If there are no transformations on any column in any function then we should be able to pass all column names as strings.\n",
    "* If not, we need to pass all columns as type column by col function.\n",
    "* If we need to apply any transformation using functions then passing column names as strings to some of the functions will not suffice. We have to pass them as column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b968f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(upper(col(\"f_name\")), upper(col(\"l_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be5c8b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Approach 1\n",
    "employeeDF.select(upper(employeeDF[\"f_name\"]), upper(employeeDF[\"l_name\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7626ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|upper(f_name)|upper(l_name)|\n",
      "+-------------+-------------+\n",
      "|        SCOTT|        TIGER|\n",
      "|        HENRY|         FORD|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternate Approach 2\n",
    "employeeDF.select(upper(employeeDF[\"f_name\"]), upper(employeeDF[\"l_name\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200da170",
   "metadata": {},
   "source": [
    "* Also, if we want to use functions such as `alias`, `desc` etc on columns then we have to pass the column anmes as column type(not as strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88b4bb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9628/2895053244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memployeeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(\"id\".desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "863817cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "| id|f_name|l_name|salary|nationality|          ph_no|        ssn|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "|  2| Henry|  Ford| 750.0|         UK|+1 123 456 7890|123 45 6789|\n",
      "|  1| Scott| Tiger|1000.0|        USA|+1 123 456 7890|123 45 6789|\n",
      "+---+------+------+------+-----------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.orderBy(col(\"id\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf4a37",
   "metadata": {},
   "source": [
    "* Sometimes, we want to add a literal to the column values. For e.g., we might want to concatenate first_name and last_name seperated by comma and space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bd21d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`, `' given input columns: [f_name, id, l_name, nationality, ph_no, salary, ssn];;\n'Project [concat(f_name#447, ', , l_name#448) AS concat(f_name, , , l_name)#649]\n+- LogicalRDD [id#446, f_name#447, l_name#448, salary#449, nationality#450, ph_no#451, ssn#452], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9628/2027930771.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memployeeDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"f_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"l_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`, `' given input columns: [f_name, id, l_name, nationality, ph_no, salary, ssn];;\n'Project [concat(f_name#447, ', , l_name#448) AS concat(f_name, , , l_name)#649]\n+- LogicalRDD [id#446, f_name#447, l_name#448, salary#449, nationality#450, ph_no#451, ssn#452], false\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(concat(col(\"f_name\"), \", \", col(\"l_name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1021f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   full_name|\n",
      "+------------+\n",
      "|Scott, Tiger|\n",
      "| Henry, Ford|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDF.select(concat(col(\"f_name\"), lit(\", \"), col(\"l_name\")).alias('full_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0fa17",
   "metadata": {},
   "source": [
    "### String manipulation functions - 1<a id=\"fifth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741791df",
   "metadata": {},
   "source": [
    "Let us understand how to concatenate strings using concat function.\n",
    "\n",
    "* Concatenating strings\n",
    "    * We can pass a variable number of strings to `concat` function.\n",
    "    * It will return one string concatenating all the strings.\n",
    "    * If we have to concatenate literal in between then we have to use lit function.\n",
    "\n",
    "* Case Conversion and Length\n",
    "    * Convert all the alphabetic characters in a string to uppercase - `upper`\n",
    "    * Convert all the alphabetic characters in a string to lowercase - `lower`\n",
    "    * Convert first character in a string to uppercase - `initcap`\n",
    "    * Get number of characters in a string - `length`\n",
    "    * All the 4 functions take column type argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7366efc",
   "metadata": {},
   "source": [
    "#### Concatenation\n",
    "Let us perform few tasks to understand more about concat function.\n",
    "\n",
    "* Letâ€™s create a Data Frame and explore concat function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e337a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d903491",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fffea62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708c378",
   "metadata": {},
   "source": [
    "* Create a new column by name `full_name` concatenating `first_name` and `last_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7691ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn| full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|ScottTiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| HenryFord|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|NickJunior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| BillGomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b962e",
   "metadata": {},
   "source": [
    "* Improvise by adding a **comma followed by a space** in between **first_name** and **last_name**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6538eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|   full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott, Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry, Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick, Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill, Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", lit(', '), \"last_name\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a33ec6",
   "metadata": {},
   "source": [
    "#### Case Conversion and Length\n",
    "\n",
    "* Use employees data and create a Data Frame.\n",
    "* Apply all 4 functions on **nationality** and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d2a5032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| united states|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|united KINGDOM|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     AUSTRALIA|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'nationality') \\\n",
    ".withColumn(\"nationality_upper\", upper(col('nationality'))) \\\n",
    ".withColumn(\"nationality_lower\", lower(col('nationality'))) \\\n",
    ".withColumn(\"nationality_initcap\", initcap(col('nationality'))) \\\n",
    ".withColumn(\"nationality_length\", length(col('nationality'))) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89753c67",
   "metadata": {},
   "source": [
    "####  Substring\n",
    "\n",
    "Let us understand how we can extract substrings using function  `substring`\n",
    "* If we are processing **fixed length columns** then we use `substring` to extract the information.\n",
    "* Here are some of the examples for **fixed length columns** and the use cases for which we typically extract information.\n",
    "* 9 Digit Social Security Number. We typically extract last 4 digits and provide it to the tele verification applications.\n",
    "* 16 Digit Credit Card Number. We typically use first 4 digit number to identify Credit Card Provider and last 4 digits for the purpose of tele verification.\n",
    "* Data coming from MainFrames systems are quite often fixed length. We might have to extract the information and store in multiple columns.\n",
    "* `substring` function takes 3 arguments, **column**, **position**, **length**. We can also provide position from the end by passing negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21779349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str, pos, len)\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. note:: The position is not zero based, but 1 based index.\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bf5f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X', )]\n",
    "df = spark.createDataFrame(l, \"dummy STRING\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3141d589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), 7, 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46b6a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), -5, 5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301e397",
   "metadata": {},
   "source": [
    "#### Tasks - substring\n",
    "\n",
    "Let us perform few tasks to extract information from fixed length strings.\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "99d20684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'phone_number', 'ssn'). \\\n",
    "            withColumn('phone_last4', substring(col('phone_number'), -4, 4).cast(\"int\")). \\\n",
    "            withColumn('ssn_last4', substring(col('ssn'), -4, 4).cast(\"int\")). \\\n",
    "            show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cbdbe2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      " |-- phone_last4: integer (nullable = true)\n",
      " |-- ssn_last4: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select('employee_id', 'phone_number', 'ssn'). \\\n",
    "            withColumn('phone_last4', substring(col('phone_number'), -4, 4).cast(\"int\")). \\\n",
    "            withColumn('ssn_last4', substring(col('ssn'), -4, 4).cast(\"int\")). \\\n",
    "            printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "977fd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef2be5",
   "metadata": {},
   "source": [
    "#### Extracting Strings using split\n",
    "Let us understand how we can extract substrings using  `split`.\n",
    "\n",
    "* If we are processing **variable length columns** with **delimiter** then we use `split` to extract the information.\n",
    "* Here are some of the examples for **variable length columns** and the use cases for which we typically extract information.\n",
    "* Address where we store House Number, Street Name, City, State and Zip Code comma separated. We might want to extract City and State for demographics reports.\n",
    "* `split` takes 2 arguments, **column** and **delimiter**.\n",
    "* `split` convert each string into array and we can access the elements using index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e1c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9841463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)|\n",
      "+--------------------------------------+\n",
      "|[Hello, World,, how, are, you]        |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "810bd56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|split(Hello World, how are you,  , -1)[2]|\n",
      "+-----------------------------------------+\n",
      "|how                                      |\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")[2]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d536b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|word                          |\n",
      "+------------------------------+\n",
      "|[Hello, World,, how, are, you]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \").alias('word')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cd379",
   "metadata": {},
   "source": [
    "* Most of the problems can be solved either by using `substring` or `split`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c17eee",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "Let us perform few tasks to extract information from fixed length strings as well as delimited variable length strings.\n",
    "\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract area code and last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a7490f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111, +44 111 111 1123\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210, +61 987 654 3223\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d60ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING,\n",
    "                    phone_numbers STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a2ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|employee_id|       phone_numbers|\n",
      "+-----------+--------------------+\n",
      "|          1|     +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|\n",
      "|          4|+61 987 654 3210,...|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"phone_numbers\") \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8ab30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------------+\n",
      "|employee_id|       phone_numbers|        ssn|    phone_number|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "|          1|     +1 123 456 7890|123 45 6789| +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"phone_numbers\", \"ssn\") \\\n",
    "            .withColumn(\"phone_number\", explode(split(col(\"phone_numbers\"), \", \"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45964d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = employeesDF.select(\"employee_id\", \"phone_numbers\", \"ssn\") \\\n",
    "            .withColumn(\"phone_number\", explode(split(col(\"phone_numbers\"), \", \"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4755b786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-----------+----------------+\n",
      "|employee_id|       phone_numbers|        ssn|    phone_number|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "|          1|     +1 123 456 7890|123 45 6789| +1 123 456 7890|\n",
      "|          2|    +91 234 567 8901|456 78 9123|+91 234 567 8901|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1111|\n",
      "|          3|+44 111 111 1111,...|222 33 4444|+44 111 111 1123|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3210|\n",
      "|          4|+61 987 654 3210,...|789 12 6118|+61 987 654 3223|\n",
      "+-----------+--------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dfebe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "|employee_id|        ssn|    phone_number|area_code|last4_phone_number|last4_ssn|\n",
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "|          1|123 45 6789| +1 123 456 7890|      123|              7890|     6789|\n",
      "|          2|456 78 9123|+91 234 567 8901|      234|              8901|     9123|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1111|      111|              1111|     4444|\n",
      "|          3|222 33 4444|+44 111 111 1123|      111|              1123|     4444|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3210|      987|              3210|     6118|\n",
      "|          4|789 12 6118|+61 987 654 3223|      987|              3223|     6118|\n",
      "+-----------+-----------+----------------+---------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\"employee_id\", \"ssn\", \"phone_number\") \\\n",
    "            .withColumn(\"area_code\", split(col(\"phone_number\"), \" \")[1].cast(\"int\")) \\\n",
    "            .withColumn(\"last4_phone_number\", split(col(\"phone_number\"), \" \")[3].cast(\"int\")) \\\n",
    "            .withColumn(\"last4_ssn\", split(col(\"ssn\"), \" \")[2].cast(\"int\")) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d46d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|employee_id|count|\n",
      "+-----------+-----+\n",
      "|          1|    1|\n",
      "|          3|    8|\n",
      "|          4|    8|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF \\\n",
    "    .groupBy('employee_id') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8737842",
   "metadata": {},
   "source": [
    "### String manipulation functions - 2<a id=\"sixth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db4fde",
   "metadata": {},
   "source": [
    "#### String Manipulation - Padding\n",
    "Let us understand how to pad characters at the beginning or at the end of strings.\n",
    "* We typically pad characters to build fixed length values or records.\n",
    "* Fixed length values or records are extensively used in Mainframes based systems.\n",
    "* Length of each and every field in fixed length records is predetermined and if the value of the field is less than the predetermined length then we pad with a standard character.\n",
    "* In terms of numeric fields we pad with zero on the leading or left side. For non numeric fields, we pad with some standard character on leading or trailing side.\n",
    "* We use `lpad` to pad a string with a specific character on leading or left side and `rpad` to pad on trailing or right side.\n",
    "* Both lpad and rpad, take 3 arguments - column or expression, desired length and the character need to be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f9d04",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform simple tasks to understand the syntax of `lpad` or `rpad`.\n",
    "* Create a Dataframe with single value and single column.\n",
    "* Apply `lpad` to pad with - to Hello to make it 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e009b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99103fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     dummy|\n",
      "+----------+\n",
      "|-----hello|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lpad(lit('hello'), 10, '-').alias('dummy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c22fd",
   "metadata": {},
   "source": [
    "* Letâ€™s take the **employees** Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c52f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "529af43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark.createDataFrame(employees). \\\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4235fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa534e9",
   "metadata": {},
   "source": [
    "* Use **pad** functions to convert each of the field into fixed length and concatenate. Here are the details for each of the fields.\n",
    " * Length of the employee_id should be 5 characters and should be padded with zero.\n",
    " * Length of first_name and last_name should be 10 characters and should be padded with - on the right side.\n",
    " * Length of salary should be 10 characters and should be padded with zero.\n",
    " * Length of the nationality should be 15 characters and should be padded with - on the right side.\n",
    " * Length of the phone_number should be 17 characters and should be padded with - on the right side.\n",
    " * Length of the ssn can be left as is. It is 11 characters.\n",
    " \n",
    "* Create a new Dataframe **empFixedDF** with column name **employee**. Preview the data by disabling truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cc0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|employee                                                                 |\n",
      "+-------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----1000.united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------1250.India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----1500.AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.select(\n",
    "                concat(\n",
    "                    lpad(col('employee_id'), 5, '0'),\n",
    "                    rpad(col('first_name'), 10, '-'),\n",
    "                    rpad(col('last_name'), 10, '-'),\n",
    "                    lpad(col('salary'), 5, '0'),\n",
    "                    rpad(col('nationality'), 15, '-'),\n",
    "                    rpad(col('phone_number'), 17, '-'),\n",
    "                    'ssn').alias('employee')\n",
    "            ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c44bc",
   "metadata": {},
   "source": [
    "#### String Manipulation - Trimming\n",
    "Let us understand how to trim unwanted leading and trailing characters around a string.\n",
    "* We typically use trimming to remove unnecessary characters from fixed length records.\n",
    "* Fixed length records are extensively used in Mainframes and we might have to process it using Spark.\n",
    "* As part of processing we might want to remove leading or trailing characters such as 0 in case of numeric types and space or some standard character in case of alphanumeric types.\n",
    "* As of now Spark trim functions take the column as argument and remove leading or trailing spaces.\n",
    "* Trim spaces towards left - `ltrim`\n",
    "* Trim spaces towards right - `rtrim`\n",
    "* Trim spaces on both sides - `trim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2017851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         dummy|\n",
      "+--------------+\n",
      "|   Hello,     |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('   Hello,     ',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae4e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+------+\n",
      "|         dummy|      ltrim|    rtrim|  trim|\n",
      "+--------------+-----------+---------+------+\n",
      "|   Hello,     |Hello,     |   Hello,|Hello,|\n",
      "+--------------+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pyspark trim only remove spaces while sql style trim functions can trim any character\n",
    "df.withColumn('ltrim', ltrim(col('dummy'))) \\\n",
    "    .withColumn('rtrim', rtrim(col('dummy'))) \\\n",
    "    .withColumn('trim', trim(col('dummy'))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e48c4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+------+\n",
      "|         dummy|      ltrim|   rtrim|  trim|\n",
      "+--------------+-----------+--------+------+\n",
      "|   Hello,     |Hello,     |   Hello|Hello,|\n",
      "+--------------+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For using sql style trim functions, use expr  \n",
    "df.withColumn('ltrim', expr(\"ltrim(dummy)\")) \\\n",
    "    .withColumn('rtrim', expr(\"rtrim(',', rtrim(dummy))\")) \\\n",
    "    .withColumn('trim', expr(\"trim(dummy)\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73dc164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|function_desc                                                                |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|Function: rtrim                                                              |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrimRight             |\n",
      "|Usage: \n",
      "    rtrim(str) - Removes the trailing space characters from `str`.\n",
      "  |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION rtrim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0c9667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|function_desc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Function: trim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Class: org.apache.spark.sql.catalyst.expressions.StringTrim                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Usage: \n",
      "    trim(str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n",
      "\n",
      "    trim(LEADING FROM str) - Removes the leading space characters from `str`.\n",
      "\n",
      "    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n",
      "\n",
      "    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n",
      "\n",
      "    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\n",
      "\n",
      "    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\n",
      "  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FUNCTION trim\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23747013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+------+\n",
      "|         dummy|      ltrim|   rtrim|  trim|\n",
      "+--------------+-----------+--------+------+\n",
      "|   Hello,     |Hello,     |   Hello|Hello,|\n",
      "+--------------+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('ltrim', expr(\"trim(LEADING FROM dummy)\")) \\\n",
    "    .withColumn('rtrim', expr(\"trim(TRAILING ',' FROM rtrim(dummy))\")) \\\n",
    "    .withColumn('trim', expr(\"trim(BOTH FROM dummy)\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd1ed7",
   "metadata": {},
   "source": [
    "### Date and Time overview<a id=\"seventh\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bedd31",
   "metadata": {},
   "source": [
    "* We can use `current_date` to get todayâ€™s server date. \n",
    " * Date will be returned using **yyyy-MM-dd** format.\n",
    "* We can use `current_timestamp` to get current server time. \n",
    " * Timestamp will be returned using **yyyy-MM-dd HH:mm:ss:SSS** format.\n",
    " * Hours will be by default in 24 hour format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b0b3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('X',)]\n",
    "df = spark.createDataFrame(l, 'dummy STRING')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d31c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-01-29|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c90592e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp()       |\n",
      "+--------------------------+\n",
      "|2022-01-29 11:45:49.100927|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d6fd01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-12-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(lit('20211201'), 'yyyyMMdd').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84e4d2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2021-12-01 17:25:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('20211201 1725'), 'yyyyMMdd HHmm').alias('timestamp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0483e",
   "metadata": {},
   "source": [
    "### Date and Time arithmetic<a id=\"eigth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bdaaf",
   "metadata": {},
   "source": [
    "Let us perform Date and Time Arithmetic using relevant functions.\n",
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting a number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e58ad2",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "Let us perform some tasks related to date arithmetic.\n",
    "* Get help on each and every function first and understand what all arguments need to be passed.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fdc1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = [(\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f4d767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF = spark.createDataFrame(datetimes, \"date STRING, time STRING\")\n",
    "datetimesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44464537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5954d8",
   "metadata": {},
   "source": [
    "### Date and Time - trunc and date_trunc<a id=\"ninth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09269e5f",
   "metadata": {},
   "source": [
    "### Date and Time - extracting information<a id=\"tenth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82512f7b",
   "metadata": {},
   "source": [
    "### Dealing with UNIX timestamp<a id=\"eleventh\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329161c",
   "metadata": {},
   "source": [
    "### Example - word count<a id=\"twelth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b920c0",
   "metadata": {},
   "source": [
    "### Conclusion<a id=\"thirteenth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1621205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
